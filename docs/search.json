[{"path":"https://yuyangyy.com/glmtlp/articles/glmtlp.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"An Introduction to glmtlp","text":"glmtlp fits generalized linear models via penalized maximum likelihood. currently supports linear logistic regression models. regularization path computed l0, l1, TLP penalty grid values regularization parameter lambda λ\\lambda (l1 TLP penalty) kappa κ\\kappa (l0 penalty). addition, package provides methods prediction plotting, functions cross-validation. authors glmtlp Chunlin Li, Yu Yang, Chong Wu, R package maintained Chunlin Li Yu Yang. Python version development. vignette describes basic usage glmtlp R.","code":""},{"path":"https://yuyangyy.com/glmtlp/articles/glmtlp.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"An Introduction to glmtlp","text":"Install package CRAN.","code":"install.packages(\"glmtlp\")"},{"path":"https://yuyangyy.com/glmtlp/articles/glmtlp.html","id":"quick-start","dir":"Articles","previous_headings":"","what":"Quick Start","title":"An Introduction to glmtlp","text":"section, go main functions outputs package. First, load glmtlp package: load simulated data set continuous response illustrate usage linear regression. fit three models calling glmtlp X, y, family=\"gaussian\" three different penalty. returned fit object class glmtlp contains relevant information fitted model use. Users can apply plot, coef predict methods fitted objects get detailed results. can visualize coefficients solution path executing plot method. output ggplot object. Therefore, users allowed customize plot suit needs. plot shows solution path model, curve corresponding variable. Users may also choose annotate curves setting label=TRUE. xvar index variable plot . Note “l1” “tlp” penalty, xvar chosen c(\"lambda\", \"log_lambda\", \"deviance\", \"l1_norm\"), “l0” penalty, xvar chosen \"kappa\".  can use coef function obtain fitted coefficients. default, results matrix, column representing coefficients every λ\\lambda κ\\kappa. users may also choose input desired value λ\\lambda κ\\kappa. Note user-supplied λ\\lambda κ\\kappa parameter range parameter sequence used fitted model. addition, can make predictions applying predict method. , users need input design matrix type prediction made. Also, users can provide desired level regularization parameters indices parameter sequence. neither provided, prediction made whole lambda kappa sequence. Cross-validation can implemented cv.glmtlp find best regularization parameter. cv.glmtlp returns cv.glmtlp object, list ingredients cross-validated fit. Users may use coef, predict, plot check cross-validation results. plot method plot deviance parameter sequence. vertical dashed line shows position index smallest CV error achieved, users may also choose omit setting vertical.line = FALSE. , output ggplot object, users free make modifications .  coef predict method default use parameter gives smallest CV error, namely, = cv.fit$idx.min.","code":"library(glmtlp) data(gau_data) X <- gau_data$X y <- gau_data$y fit <- glmtlp(X, y, family = \"gaussian\", penalty = \"tlp\") fit2 <- glmtlp(X, y, family = \"gaussian\", penalty = \"l0\") fit3 <- glmtlp(X, y, family = \"gaussian\", penalty = \"l1\") plot(fit, xvar = \"lambda\") coef(fit) ... ##              1.32501    1.23571    1.15242     1.07475     1.00232     0.93477 ## intercept -0.2428484 -0.1352757 -0.0235558 -0.02355059 -0.05190855 -0.05192106 ## V1         0.0000000  1.2224980  1.2400538  1.24005913  1.27181076  1.27178077 ## V2         0.0000000  0.0000000  0.0000000  0.00000000  0.00000000  0.00000000 ## V3         0.0000000  0.0000000  0.0000000  0.00000000  0.00000000  0.00000000 ## V4         0.0000000  0.0000000  0.0000000  0.00000000  0.00000000  0.00000000 ## V5         0.0000000  0.0000000  0.0000000  0.00000000  0.00000000  0.00000000 ## V6         0.0000000  0.0000000  0.0000000  0.00000000  0.93236905  0.93231995 ## V7         0.0000000  0.0000000  0.0000000  0.00000000  0.00000000  0.00000000 ## V8         0.0000000  0.0000000  0.0000000  0.00000000  0.00000000  0.00000000 ... coef(fit, lambda = 0.1) ... ##   intercept          V1          V2          V3          V4          V5  ##  0.03012329  1.25295108  0.00000000 -0.18639467 -0.15726983 -0.19310409  ##          V6          V7          V8          V9         V10         V11  ##  0.91543631  0.00000000  0.01275199  0.00000000  0.70521331  0.19432176  ##         V12         V13         V14         V15         V16         V17  ##  0.01640360  0.00000000  0.17320713  1.16204702  0.00000000  0.00000000  ##         V18         V19         V20  ##  0.00000000 -0.19471461  0.94229082 NA NA ... predict(fit, X[1:5, ], lambda = 0.1) ## [1]  0.09972438  2.66195238 -1.33516956  0.33721013 -2.63615326 predict(fit, X[1:5, ], which = 10) # the 10th lambda in the lambda sequence ## [1]  0.1906092  2.2279251 -1.4255474  0.9313526 -2.8151620 cv.fit <- cv.glmtlp(X, y, family = \"gaussian\", penalty = \"tlp\") plot(cv.fit) coef(cv.fit) ##    intercept           V1           V2           V3           V4           V5  ## -0.009695669  1.240195285  0.000000000  0.000000000  0.000000000  0.000000000  ##           V6           V7           V8           V9          V10          V11  ##  0.883162055  0.000000000  0.000000000  0.000000000  0.725706691  0.000000000  ##          V12          V13          V14          V15          V16          V17  ##  0.000000000  0.000000000  0.000000000  1.125977511  0.000000000  0.000000000  ##          V18          V19          V20  ##  0.000000000  0.000000000  0.981387932 predict(cv.fit, X[1:5, ]) ## [1]  0.1906096  2.2279220 -1.4255447  0.9313487 -2.8151577"},{"path":"https://yuyangyy.com/glmtlp/articles/glmtlp.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"An Introduction to glmtlp","text":"Li, C., Shen, X., & Pan, W. (2021). Inference large directed graphical model interventions. arXiv preprint arXiv:2110.03805. https://arxiv.org/abs/2110.03805. Shen, X., Pan, W., & Zhu, Y. (2012). Likelihood-based selection sharp parameter estimation. Journal American Statistical Association, 107(497), 223-232. https://doi.org/10.1080/01621459.2011.645783. Shen, X., Pan, W., Zhu, Y., & Zhou, H. (2013). constrained regularized high-dimensional regression. Annals Institute Statistical Mathematics, 65(5), 807-832. https://doi.org/10.1007/s10463-012-0396-3. Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon, N., Taylor, J., & Tibshirani, R. J. (2012). Strong rules discarding predictors lasso‐type problems. Journal Royal Statistical Society: Series B (Statistical Methodology), 74(2), 245-266. https://doi.org/10.1111/j.1467-9868.2011.01004.x. Yang, Y. & Zou, H. coordinate majorization descent algorithm l1 penalized learning. Journal Statistical Computation Simulation 84.1 (2014): 84-95. https://doi.org/10.1080/00949655.2012.695374. Zhu, Y., Shen, X., & Pan, W. (2020). high-dimensional constrained maximum likelihood inference. Journal American Statistical Association, 115(529), 217-230. https://doi.org/10.1080/01621459.2018.1540986. Zhu, Y. (2017). augmented ADMM algorithm application generalized lasso problem. Journal Computational Graphical Statistics, 26(1), 195-204. https://doi.org/10.1080/10618600.2015.1114491.","code":""},{"path":"https://yuyangyy.com/glmtlp/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Chunlin Li. Author, copyright holder. Yu Yang. Author, maintainer, copyright holder. Chong Wu. Author, copyright holder. Xiaotong Shen. Thesis advisor, copyright holder. Wei Pan. Thesis advisor, copyright holder.","code":""},{"path":"https://yuyangyy.com/glmtlp/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Li C, Yang Y, Wu C (2024). glmtlp: Generalized Linear Models Truncated Lasso Penalty. R package version 2.0.2, https://yuyangyy.com/glmtlp/.","code":"@Manual{,   title = {glmtlp: Generalized Linear Models with Truncated Lasso Penalty},   author = {Chunlin Li and Yu Yang and Chong Wu},   year = {2024},   note = {R package version 2.0.2},   url = {https://yuyangyy.com/glmtlp/}, }"},{"path":"https://yuyangyy.com/glmtlp/index.html","id":"glmtlp-an-r-package-for-truncated-lasso-penalty","dir":"","previous_headings":"","what":"Generalized Linear Models with Truncated Lasso Penalty","title":"Generalized Linear Models with Truncated Lasso Penalty","text":"Efficient procedures constrained likelihood estimation truncated lasso penalty (Shen et al., 2010; Zhang 2010) linear generalized linear models. Note: repo version published CRAN. Please check chunlinli/glmtlp new features constrained likelihood inference, regression summary data, memory efficiency, Gaussian graphical models, .","code":""},{"path":"https://yuyangyy.com/glmtlp/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Generalized Linear Models with Truncated Lasso Penalty","text":"can install released version glmtlp CRAN :","code":"install.packages(\"glmtlp\")"},{"path":"https://yuyangyy.com/glmtlp/index.html","id":"examples-for-gaussian-regression-models","dir":"","previous_headings":"","what":"Examples for Gaussian Regression Models","title":"Generalized Linear Models with Truncated Lasso Penalty","text":"following three examples show use glmtlp fit gaussian regression models:","code":"library(glmtlp) data(\"gau_data\") colnames(gau_data$X)[gau_data$beta != 0] #> [1] \"V1\"  \"V6\"  \"V10\" \"V15\" \"V20\" # Cross-Validation using TLP penalty cv.fit <- cv.glmtlp(gau_data$X, gau_data$y, family = \"gaussian\", penalty = \"tlp\", ncores=2) coef(cv.fit)[abs(coef(cv.fit)) > 0] #>    intercept           V1           V6          V10          V15          V20  #> -0.009678041  1.240223517  0.883202180  0.725708239  1.125994003  0.981402236 plot(cv.fit) # Single Model Fit using TLP penalty fit <- glmtlp(gau_data$X, gau_data$y, family = \"gaussian\", penalty = \"tlp\") coef(fit, lambda = cv.fit$lambda.min) #>    intercept           V1           V2           V3           V4           V5  #> -0.009678041  1.240223517  0.000000000  0.000000000  0.000000000  0.000000000  #>           V6           V7           V8           V9          V10          V11  #>  0.883202180  0.000000000  0.000000000  0.000000000  0.725708239  0.000000000  #>          V12          V13          V14          V15          V16          V17  #>  0.000000000  0.000000000  0.000000000  1.125994003  0.000000000  0.000000000  #>          V18          V19          V20  #>  0.000000000  0.000000000  0.981402236 predict(fit, X = gau_data$X[1:5, ], lambda = cv.fit$lambda.min) #> [1]  0.1906465  2.2279723 -1.4256042  0.9313886 -2.8152522 plot(fit, xvar = \"log_lambda\", label = TRUE) # Cross-Validation using L0 penalty cv.fit <- cv.glmtlp(gau_data$X, gau_data$y, family = \"gaussian\", penalty = \"l0\", ncores=2) coef(cv.fit)[abs(coef(cv.fit)) > 0] #>    intercept           V1           V6          V10          V15          V20  #> -0.009687042  1.240319880  0.883378583  0.725607300  1.125958218  0.981544178 plot(cv.fit) # Single Model Fit using L0 penalty fit <- glmtlp(gau_data$X, gau_data$y, family = \"gaussian\", penalty = \"l0\") coef(fit, kappa = cv.fit$kappa.min) #>    intercept           V1           V2           V3           V4           V5  #> -0.009687042  1.240319880  0.000000000  0.000000000  0.000000000  0.000000000  #>           V6           V7           V8           V9          V10          V11  #>  0.883378583  0.000000000  0.000000000  0.000000000  0.725607300  0.000000000  #>          V12          V13          V14          V15          V16          V17  #>  0.000000000  0.000000000  0.000000000  1.125958218  0.000000000  0.000000000  #>          V18          V19          V20  #>  0.000000000  0.000000000  0.981544178 predict(fit, X = gau_data$X[1:5, ], kappa = cv.fit$kappa.min) #> [1]  0.190596  2.228306 -1.425994  0.931749 -2.815322 plot(fit, xvar = \"kappa\", label = TRUE) # Cross-Validation using L1 penalty cv.fit <- cv.glmtlp(gau_data$X, gau_data$y, family = \"gaussian\", penalty = \"l1\", ncores=2) coef(cv.fit)[abs(coef(cv.fit)) > 0] #>   intercept          V1          V3          V4          V5          V6  #> -0.01185622  1.16222899 -0.06606911 -0.08387185 -0.06870578  0.79106593  #>          V8          V9         V10         V11         V14         V15  #>  0.01136376  0.01038075  0.62580166  0.10858744  0.08533479  1.04737369  #>         V19         V20  #> -0.11859786  0.86736897 plot(cv.fit) # Single Model Fit using L1 penalty fit <- glmtlp(gau_data$X, gau_data$y, family = \"gaussian\", penalty = \"l1\") coef(fit, lambda = cv.fit$lambda.min) #>   intercept          V1          V2          V3          V4          V5  #> -0.01185622  1.16222899  0.00000000 -0.06606911 -0.08387185 -0.06870578  #>          V6          V7          V8          V9         V10         V11  #>  0.79106593  0.00000000  0.01136376  0.01038075  0.62580166  0.10858744  #>         V12         V13         V14         V15         V16         V17  #>  0.00000000  0.00000000  0.08533479  1.04737369  0.00000000  0.00000000  #>         V18         V19         V20  #>  0.00000000 -0.11859786  0.86736897 predict(fit, X = gau_data$X[1:5, ], lambda = cv.fit$lambda.min) #> [1]  0.07112074  2.17093497 -1.09936871  0.46108771 -2.25111685 plot(fit, xvar = \"lambda\", label = TRUE)"},{"path":"https://yuyangyy.com/glmtlp/index.html","id":"examples-for-logistic-regression-models","dir":"","previous_headings":"","what":"Examples for Logistic Regression Models","title":"Generalized Linear Models with Truncated Lasso Penalty","text":"following three examples show use glmtlp fit logistic regression models:","code":"data(\"bin_data\") colnames(bin_data$X)[bin_data$beta != 0] #> [1] \"V1\"  \"V6\"  \"V10\" \"V15\" \"V20\" # Cross-Validation using TLP penalty cv.fit <- cv.glmtlp(bin_data$X, bin_data$y, family = \"binomial\", penalty = \"tlp\", ncores=2) coef(cv.fit)[abs(coef(cv.fit)) > 0] #>  intercept         V6        V20  #> -0.1347141  0.8256183  0.9940325 plot(cv.fit) #> Warning: Removed 98 rows containing missing values or values outside the scale range #> (`geom_line()`). #> Warning: Removed 98 rows containing missing values or values outside the scale range #> (`geom_point()`). # Single Model Fit using TLP penalty fit <- glmtlp(bin_data$X, bin_data$y, family = \"binomial\", penalty = \"tlp\") coef(fit, lambda = cv.fit$lambda.min) #>  intercept         V1         V2         V3         V4         V5         V6  #> -0.1347141  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.8256183  #>         V7         V8         V9        V10        V11        V12        V13  #>  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  #>        V14        V15        V16        V17        V18        V19        V20  #>  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.9940325 predict(fit, X = bin_data$X[1:5, ], type = \"response\", lambda = cv.fit$lambda.min) #> [1] 0.42562483 0.89838483 0.09767039 0.90898462 0.20822294 plot(fit, xvar = \"log_lambda\", label = TRUE) # Cross-Validation using L0 penalty cv.fit <- cv.glmtlp(bin_data$X, bin_data$y, family = \"binomial\", penalty = \"l0\", ncores=2) coef(cv.fit)[abs(coef(cv.fit)) > 0] #>  intercept         V6        V20  #> -0.1347137  0.8256471  0.9940180 plot(cv.fit) # Single Model Fit using L0 penalty fit <- glmtlp(bin_data$X, bin_data$y, family = \"binomial\", penalty = \"l0\") coef(fit, kappa = cv.fit$kappa.min) #>  intercept         V1         V2         V3         V4         V5         V6  #> -0.1347137  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.8256471  #>         V7         V8         V9        V10        V11        V12        V13  #>  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  #>        V14        V15        V16        V17        V18        V19        V20  #>  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.0000000  0.9940180 predict(fit, X = bin_data$X[1:5, ], kappa = cv.fit$kappa.min) #> [1] -0.2996886  2.1793764 -2.2234461  2.3012922 -1.3357999 plot(fit, xvar = \"kappa\", label = TRUE) # Cross-Validation using L1 penalty cv.fit <- cv.glmtlp(bin_data$X, bin_data$y, family = \"binomial\", penalty = \"l1\", ncores=2) coef(cv.fit)[abs(coef(cv.fit)) > 0] #>   intercept          V1          V3          V4          V5          V6  #> -0.04597434  0.74281436  0.04345031  0.15993696  0.05100859  0.98672196  #>          V8          V9         V10         V13         V15         V19  #> -0.04488821 -0.06456282  0.66422939  0.33826482  0.69062166  0.23686317  #>         V20  #>  1.01116571 plot(cv.fit) # Single Model Fit using L1 penalty fit <- glmtlp(bin_data$X, bin_data$y, family = \"binomial\", penalty = \"l1\") coef(fit, lambda = cv.fit$lambda.min) #>   intercept          V1          V2          V3          V4          V5  #> -0.04597434  0.74281436  0.00000000  0.04345031  0.15993696  0.05100859  #>          V6          V7          V8          V9         V10         V11  #>  0.98672196  0.00000000 -0.04488821 -0.06456282  0.66422939  0.00000000  #>         V12         V13         V14         V15         V16         V17  #>  0.00000000  0.33826482  0.00000000  0.69062166  0.00000000  0.00000000  #>         V18         V19         V20  #>  0.00000000  0.23686317  1.01116571 predict(fit, X = bin_data$X[1:5, ], type = \"response\", lambda = cv.fit$lambda.min) #> [1] 0.35132374 0.90851038 0.03822033 0.93657911 0.03253188 plot(fit, xvar = \"lambda\", label = TRUE)"},{"path":"https://yuyangyy.com/glmtlp/index.html","id":"citing-information","dir":"","previous_headings":"","what":"Citing information","title":"Generalized Linear Models with Truncated Lasso Penalty","text":"find project useful, please consider citing","code":"@article{     author = {Chunlin Li, Yu Yang, Chong Wu, Xiaotong Shen, Wei Pan},     title = {{glmtlp: An R package for truncated Lasso penalty}},     year = {2022} }"},{"path":"https://yuyangyy.com/glmtlp/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Generalized Linear Models with Truncated Lasso Penalty","text":"Li, C., Shen, X., & Pan, W. (2021). Inference large directed graphical model interventions. arXiv preprint arXiv:2110.03805. https://arxiv.org/abs/2110.03805. Shen, X., Pan, W., & Zhu, Y. (2012). Likelihood-based selection sharp parameter estimation. Journal American Statistical Association, 107(497), 223-232. https://doi.org/10.1080/01621459.2011.645783. Shen, X., Pan, W., Zhu, Y., & Zhou, H. (2013). constrained regularized high-dimensional regression. Annals Institute Statistical Mathematics, 65(5), 807-832. https://doi.org/10.1007/s10463-012-0396-3. Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon, N., Taylor, J., & Tibshirani, R. J. (2012). Strong rules discarding predictors lasso‐type problems. Journal Royal Statistical Society: Series B (Statistical Methodology), 74(2), 245-266. https://doi.org/10.1111/j.1467-9868.2011.01004.x. Yang, Y. & Zou, H. coordinate majorization descent algorithm l1 penalized learning. Journal Statistical Computation Simulation 84.1 (2014): 84-95. https://doi.org/10.1080/00949655.2012.695374. Zhu, Y., Shen, X., & Pan, W. (2020). high-dimensional constrained maximum likelihood inference. Journal American Statistical Association, 115(529), 217-230. https://doi.org/10.1080/01621459.2018.1540986. Zhu, Y. (2017). augmented ADMM algorithm application generalized lasso problem. Journal Computational Graphical Statistics, 26(1), 195-204. https://doi.org/10.1080/10618600.2015.1114491. Part code adapted glmnet, ncvreg, biglasso. Warm thanks authors open-sourced softwares.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/bin_data.html","id":null,"dir":"Reference","previous_headings":"","what":"A simulated binomial data set. — bin_data","title":"A simulated binomial data set. — bin_data","text":"data set simulated illustrating logistic regression models. Generated   gen.binomial.data(n = 200, p = 20, seed = 2021).","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/bin_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A simulated binomial data set. — bin_data","text":"","code":"data(bin_data)"},{"path":"https://yuyangyy.com/glmtlp/reference/bin_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"A simulated binomial data set. — bin_data","text":"list three elements: design matrix X, response y,   true coefficient vector beta. X design matrix y response beta true coefficient vector","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/bin_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A simulated binomial data set. — bin_data","text":"","code":"data(\"bin_data\") cv.fit <- cv.glmtlp(bin_data$X, bin_data$y, family = \"binomial\", penalty = \"l1\") plot(cv.fit)"},{"path":"https://yuyangyy.com/glmtlp/reference/cv.glmtlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-validation for glmtlp — cv.glmtlp","title":"Cross-validation for glmtlp — cv.glmtlp","text":"Performs k-fold cross-validation l0, l1, TLP-penalized regression models grid values regularization parameter lambda (penalty=\"l0\") kappa (penalty=\"l0\").","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/cv.glmtlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-validation for glmtlp — cv.glmtlp","text":"","code":"cv.glmtlp(X, y, ..., seed = NULL, nfolds = 10, obs.fold = NULL, ncores = 1)"},{"path":"https://yuyangyy.com/glmtlp/reference/cv.glmtlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-validation for glmtlp — cv.glmtlp","text":"X input matrix, dimension nobs x nvars, glmtlp. y response, length nobs, glmtlp. ... arguments can passed glmtlp. seed seed reproduction purposes nfolds number folds; default 10. smallest value allowable nfolds=3 obs.fold optional vector values 1 nfolds identifying fold observation . supplied, nfolds can missing. ncores number cores utilized; default 1. greater 1, doParallel::foreach used fit fold; equal 1, loop used fit fold. Users register parallel clusters outside.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/cv.glmtlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-validation for glmtlp — cv.glmtlp","text":"object class \"cv.glmtlp\" returned, list   ingredients cross-validation fit. call function call cv.mean mean cross-validated error - vector length   length(kappa) penalty = \"l0\" length{lambda}   otherwise. cv.se estimate standard error cv.mean. fit fitted glmtlp object full data. idx.min index lambda kappa sequence   corresponding smallest cv mean error. kappa values kappa used fits, available   penalty = 'l0'. kappa.min value kappa gives minimum   cv.mean, available penalty = 'l0'. lambda values lambda used fits. lambda.min value lambda gives minimum cv.mean,   available penalty 'l1' 'tlp'. null.dev null deviance model. obs.fold fold id observation used CV.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/cv.glmtlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-validation for glmtlp — cv.glmtlp","text":"function calls glmtlp nfolds+1 times; first call get   lambda kappa sequence, rest compute   fit folds omitted. cross-validation error based   deviance (check details). error accumulated   folds, average error standard deviation computed. family = \"binomial\", fold assignment (provided   user) generated stratified manner, ratio 0/1 outcomes   fold.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/cv.glmtlp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Cross-validation for glmtlp — cv.glmtlp","text":"Shen, X., Pan, W., & Zhu, Y. (2012).   Likelihood-based selection sharp parameter estimation.   Journal American Statistical Association, 107(497), 223-232.    Shen, X., Pan, W., Zhu, Y., & Zhou, H. (2013).   constrained regularized high-dimensional regression.   Annals Institute Statistical Mathematics, 65(5), 807-832.    Li, C., Shen, X., & Pan, W. (2021).   Inference Large Directed Graphical Model Interventions.   arXiv preprint arXiv:2110.03805.    Yang, Y., & Zou, H. (2014).   coordinate majorization descent algorithm l1 penalized learning.   Journal Statistical Computation Simulation, 84(1), 84-95.    Two R package Github: ncvreg glmnet.","code":""},{"path":[]},{"path":"https://yuyangyy.com/glmtlp/reference/cv.glmtlp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Cross-validation for glmtlp — cv.glmtlp","text":"Chunlin Li, Yu Yang, Chong Wu    Maintainer: Yu Yang yang6367@umn.edu","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/cv.glmtlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-validation for glmtlp — cv.glmtlp","text":"","code":"# Gaussian X <- matrix(rnorm(100 * 20), 100, 20) y <- rnorm(100) cv.fit <- cv.glmtlp(X, y, family = \"gaussian\", penalty = \"l1\", seed=2021)  # Binomial X <- matrix(rnorm(100 * 20), 100, 20) y <- sample(c(0,1), 100, replace = TRUE) cv.fit <- cv.glmtlp(X, y, family = \"binomial\", penalty = \"l1\", seed=2021)"},{"path":"https://yuyangyy.com/glmtlp/reference/gau_data.html","id":null,"dir":"Reference","previous_headings":"","what":"A simulated gaussian data set. — gau_data","title":"A simulated gaussian data set. — gau_data","text":"data set simulated illustrating linear regression models. Generated   gen.gaussian.data(n = 200, p = 20, seed = 2021).","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/gau_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A simulated gaussian data set. — gau_data","text":"","code":"data(gau_data)"},{"path":"https://yuyangyy.com/glmtlp/reference/gau_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"A simulated gaussian data set. — gau_data","text":"list five elements: design matrix X, response y,   correlation structure covariates Sigma, true beta beta,   noise level sigma. X design matrix y response beta true beta values sigma noise level","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/gau_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A simulated gaussian data set. — gau_data","text":"","code":"data(\"gau_data\") cv.fit <- cv.glmtlp(gau_data$X, gau_data$y, family = \"gaussian\", penalty = \"tlp\") plot(cv.fit)"},{"path":"https://yuyangyy.com/glmtlp/reference/gen.binomial.data.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate a binomial data set — gen.binomial.data","title":"Simulate a binomial data set — gen.binomial.data","text":"Simulate data set binary response following logistic regression   model.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/gen.binomial.data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate a binomial data set — gen.binomial.data","text":"","code":"gen.binomial.data(n, p, rho = 0, kappa = 5, beta.type = 1, seed = 2021)"},{"path":"https://yuyangyy.com/glmtlp/reference/gen.binomial.data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate a binomial data set — gen.binomial.data","text":"n Sample size. p Number covariates. rho parameter defining AR(1) correlation matrix. kappa number nonzero coefficients. beta.type Numeric indicator choosing beta type. beta.type = 1, true coefficient vector kappa components 1, roughly equally distributed 1 p. beta.type = 2, first kappa values 1, rest 0. beta.type = 3, first kappa values equally-spaced values 10 0.5, rest 0. beta.type = 4, first kappa values first kappa values c(-10, -6, -2, 2, 6, 10), rest 0. beta.type = 5, first kappa values 1, rest decay exponentially 0 base 0.5. seed seed reproducibility. Default 2021.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/gen.binomial.data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate a binomial data set — gen.binomial.data","text":"list containing simulated data. X covariate matrix, dimension n x p. y response, length n. beta true coefficients, length p.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/gen.binomial.data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate a binomial data set — gen.binomial.data","text":"","code":"bin_data <- gen.binomial.data(n = 200, p = 20, seed = 2021) head(bin_data$X) #>              V1         V2         V3         V4         V5          V6 #> [1,] -0.1224600  0.2701953 -0.6745562  0.1492579  0.5534009  0.80959540 #> [2,]  0.5524566 -1.3432502  0.2381266 -0.3123636 -1.1167970  0.09941633 #> [3,]  0.3486495 -0.8488889  0.5450859 -1.2595894  0.5740307 -2.34302131 #> [4,]  0.3596322 -0.4076079 -0.4488515  0.0519813  1.2043346  0.67652598 #> [5,]  0.8980537 -0.6661505  0.9712467  0.2044272  0.7274956 -3.61147374 #> [6,] -1.9225695 -0.1032374 -1.5471639  1.3869823 -0.7023848 -0.16416799 #>               V7         V8         V9        V10        V11        V12 #> [1,] -0.89020680 -1.1028167  1.3645402  0.4982807  0.3147181 -0.6066470 #> [2,]  0.96022096  0.4343786  0.4568313  0.9032203  1.2000068  1.0565220 #> [3,]  1.26888856  1.2058157  2.1983623 -0.4181034 -0.8072166  1.3920142 #> [4,]  0.25411730 -1.0206682 -0.6805850  1.4747203 -0.6763614 -1.3792104 #> [5,]  0.03379941  1.8178839 -0.9349661 -2.0133506  0.7913844 -0.1406768 #> [6,]  0.03272896 -0.4072112  0.9655597  1.6591117  0.6367649 -1.1186803 #>             V13        V14         V15         V16         V17       V18 #> [1,] -2.8645250 -0.4444342  0.08738679  0.07521845 -0.28434897 1.0566142 #> [2,] -0.4067150  0.8172733 -1.23844170 -0.76200580 -0.04859536 1.4287991 #> [3,] -0.9921254 -0.7647219  0.60099751 -0.82983377 -1.20895180 1.2845802 #> [4,]  1.5503138 -0.2771623 -2.68766095 -0.75836338  0.48379191 0.1668427 #> [5,] -0.1953912  1.2854050 -0.91182644 -1.55430328  2.42388384 1.2601177 #> [6,]  3.1064929 -0.9233080 -0.77375020 -1.62213762  1.74302228 0.9810522 #>             V19        V20 #> [1,]  0.7012877 -0.8384305 #> [2,]  0.4267106  2.2454396 #> [3,] -1.8019905 -0.1551519 #> [4,]  0.4479343  1.8887326 #> [5,] -1.3472316  1.7914329 #> [6,]  0.3858948  0.2903452 head(bin_data$y) #> [1] 0 1 0 1 0 1 head(bin_data$beta) #> [1] 1 0 0 0 0 1"},{"path":"https://yuyangyy.com/glmtlp/reference/gen.gaussian.data.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate a gaussian data set — gen.gaussian.data","title":"Simulate a gaussian data set — gen.gaussian.data","text":"Simulate data set gaussian response following linear regression   model.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/gen.gaussian.data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate a gaussian data set — gen.gaussian.data","text":"","code":"gen.gaussian.data(   n,   p,   rho = 0,   kappa = 5,   beta.type = 1,   snr = 1,   seed = 2021 )"},{"path":"https://yuyangyy.com/glmtlp/reference/gen.gaussian.data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate a gaussian data set — gen.gaussian.data","text":"n Sample size. p Number covariates. rho parameter defining AR(1) correlation matrix. kappa number nonzero coefficients. beta.type Numeric indicator choosing beta type. beta.type = 1, true coefficient vector kappa components 1, roughly equally distributed 1 p. beta.type = 2, first kappa values 1, rest 0. beta.type = 3, first kappa values equally-spaced values 10 0.5, rest 0. beta.type = 4, first kappa values first kappa values c(-10, -6, -2, 2, 6, 10), rest 0. beta.type = 5, first kappa values 1, rest decay exponentially 0 base 0.5. snr Signal--noise ratio. Default 1. seed seed reproducibility. Default 2021.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/gen.gaussian.data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate a gaussian data set — gen.gaussian.data","text":"list containing simulated data. X covariate matrix, dimension n x p. y response, length n. beta true coefficients, length p. sigma standard error noise.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/gen.gaussian.data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate a gaussian data set — gen.gaussian.data","text":"","code":"gau_data <- gen.gaussian.data(n = 200, p = 20, seed = 2021) head(gau_data$X) #>              V1         V2         V3         V4         V5          V6 #> [1,] -0.1224600  0.2701953 -0.6745562  0.1492579  0.5534009  0.80959540 #> [2,]  0.5524566 -1.3432502  0.2381266 -0.3123636 -1.1167970  0.09941633 #> [3,]  0.3486495 -0.8488889  0.5450859 -1.2595894  0.5740307 -2.34302131 #> [4,]  0.3596322 -0.4076079 -0.4488515  0.0519813  1.2043346  0.67652598 #> [5,]  0.8980537 -0.6661505  0.9712467  0.2044272  0.7274956 -3.61147374 #> [6,] -1.9225695 -0.1032374 -1.5471639  1.3869823 -0.7023848 -0.16416799 #>               V7         V8         V9        V10        V11        V12 #> [1,] -0.89020680 -1.1028167  1.3645402  0.4982807  0.3147181 -0.6066470 #> [2,]  0.96022096  0.4343786  0.4568313  0.9032203  1.2000068  1.0565220 #> [3,]  1.26888856  1.2058157  2.1983623 -0.4181034 -0.8072166  1.3920142 #> [4,]  0.25411730 -1.0206682 -0.6805850  1.4747203 -0.6763614 -1.3792104 #> [5,]  0.03379941  1.8178839 -0.9349661 -2.0133506  0.7913844 -0.1406768 #> [6,]  0.03272896 -0.4072112  0.9655597  1.6591117  0.6367649 -1.1186803 #>             V13        V14         V15         V16         V17       V18 #> [1,] -2.8645250 -0.4444342  0.08738679  0.07521845 -0.28434897 1.0566142 #> [2,] -0.4067150  0.8172733 -1.23844170 -0.76200580 -0.04859536 1.4287991 #> [3,] -0.9921254 -0.7647219  0.60099751 -0.82983377 -1.20895180 1.2845802 #> [4,]  1.5503138 -0.2771623 -2.68766095 -0.75836338  0.48379191 0.1668427 #> [5,] -0.1953912  1.2854050 -0.91182644 -1.55430328  2.42388384 1.2601177 #> [6,]  3.1064929 -0.9233080 -0.77375020 -1.62213762  1.74302228 0.9810522 #>             V19        V20 #> [1,]  0.7012877 -0.8384305 #> [2,]  0.4267106  2.2454396 #> [3,] -1.8019905 -0.1551519 #> [4,]  0.4479343  1.8887326 #> [5,] -1.3472316  1.7914329 #> [6,]  0.3858948  0.2903452 head(gau_data$y) #> [1]  4.828933343  3.050029776  1.688536187 -0.001211368 -3.060247660 #> [6] -0.697806550 head(gau_data$beta) #> [1] 1 0 0 0 0 1 gau_data$sigma #> [1] 2.236068"},{"path":"https://yuyangyy.com/glmtlp/reference/glmtlp.html","id":null,"dir":"Reference","previous_headings":"","what":"glmtlp: A package for fitting a GLM with l0, l1, and tlp regularization. — glmtlp","title":"glmtlp: A package for fitting a GLM with l0, l1, and tlp regularization. — glmtlp","text":"package provides 3 penalties: l0, l1, tlp 3 distribution families:   gaussian, binomial, poisson. Fit generalized linear models via penalized maximum likelihood.   regularization path computed l0, lasso, truncated lasso   penalty grid values regularization parameter lambda   kappa. Fits linear logistic regression models.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/glmtlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"glmtlp: A package for fitting a GLM with l0, l1, and tlp regularization. — glmtlp","text":"","code":"glmtlp(   X,   y,   family = c(\"gaussian\", \"binomial\"),   penalty = c(\"l0\", \"l1\", \"tlp\"),   nlambda = ifelse(penalty == \"l0\", 50, 100),   lambda.min.ratio = ifelse(nobs < nvars, 0.05, 0.001),   lambda = NULL,   kappa = NULL,   tau = 0.3 * sqrt(log(nvars)/nobs),   delta = 2,   tol = 1e-04,   weights = NULL,   penalty.factor = rep(1, nvars),   standardize = FALSE,   dc.maxit = 20,   cd.maxit = 10000,   nr.maxit = 20,   ... )"},{"path":"https://yuyangyy.com/glmtlp/reference/glmtlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"glmtlp: A package for fitting a GLM with l0, l1, and tlp regularization. — glmtlp","text":"X Input matrix, dimension nobs x nvars; row  observation vector. y Response variable, length nobs. family=\"gaussian\", quantitative; family=\"binomial\", either factor two levels binary vector. family character string representing one built-families. See Details section . penalty character string representing one built-penalties. \"l0\" represents \\(L_0\\) penalty, \"l1\" represents lasso-type penalty (\\(L_1\\) penalty), \"tlp\" represents truncated lasso penalty. nlambda number lambda values. Default 100. lambda.min.ratio smallest value lambda, fraction lambda.max, smallest value coefficients zero. default depends sample size nobs relative number variables nvars. nobs > nvars, default 0.0001, nobs < nvars, default 0.01. lambda user-supplied lambda sequence. Typically, users let program compute lambda sequence based nlambda lambda.min.ratio. Supplying value lambda override . WARNING: please use option care. glmtlp relies warms starts speed, often faster fit whole path single fit. Therefore, provide decreasing sequence lambda values want use option. Also, penalty = 'l0', recommended users supply parameter. kappa user-supplied kappa sequence. Typically, users let program compute kappa sequence based nvars nobs. sequence used penalty = 'l0'. tau tuning parameter used TLP-penalized regression models. Default  0.3 * sqrt(log(nvars)/nobs). delta tuning parameter used coordinate majorization descent algorithm. See Yang, Y., & Zou, H. (2014) reference detail. tol Tolerance level iterative optimization algorithms. weights Observation weights. Default 1 observation. penalty.factor Separate penalty factors applied coefficient, allows differential shrinkage. Default 1 variables. standardize Logical. Whether standardize input matrix X; default TRUE. dc.maxit Maximum number iterations DC (Difference Convex Functions) programming; default 20. cd.maxit Maximum number iterations coordinate descent algorithm; default 10^4. nr.maxit Maximum number iterations Newton-Raphson method; default 500. ... Additional arguments.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/glmtlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"glmtlp: A package for fitting a GLM with l0, l1, and tlp regularization. — glmtlp","text":"object S3 class \"glmtlp\". beta nvars x length(kappa) matrix   coefficients penalty = 'l0'; nvars x length(lambda)   matrix coefficients penalty = c('l1', 'tlp'). call call produces object. family distribution family used model fitting. intercept intercept vector, length(kappa)   penalty = 'l0' length(lambda)   penalty = c('l1', 'tlp'). lambda actual sequence lambda values used. Note   length may smaller provided nlambda due removal   saturated values. penalty penalty type model fitting. penalty.factor penalty factor coefficient used model fitting. tau tuning parameter used model fitting, available   penalty = 'tlp'.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/glmtlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"glmtlp: A package for fitting a GLM with l0, l1, and tlp regularization. — glmtlp","text":"sequence models indexed lambda (penalty = c('l1', 'tlp'))   kappa (penalty = 'l0') fit coordinate   descent algorithm. objective function \"gaussian\" family :   $$1/2 RSS/nobs + \\lambda*penalty,$$ models :   $$-loglik/nobs + \\lambda*penalty.$$   Also note , \"gaussian\", glmtlp standardizes y   unit variance (using 1/(n-1) formula). ## Details family option glmtlp currently supports built-families, specified   character string. families, returned object regularization   path fitting generalized linear regression models, maximizing   corresponding penalized log-likelihood. glmtlp(..., family=\"binomial\")   fits traditional logistic regression model log-odds. ## Details penalty option built-penalties specified character string. l0   penalty, kappa sequence used generating regularization   path, l1 tlp penalty, lambda sequence   used generating regularization path.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/glmtlp.html","id":"glmtlp-functions","dir":"Reference","previous_headings":"","what":"glmtlp functions","title":"glmtlp: A package for fitting a GLM with l0, l1, and tlp regularization. — glmtlp","text":"`glmtlp()`, `cv.glmtlp()`","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/glmtlp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"glmtlp: A package for fitting a GLM with l0, l1, and tlp regularization. — glmtlp","text":"Shen, X., Pan, W., & Zhu, Y. (2012).   Likelihood-based selection sharp parameter estimation.   Journal American Statistical Association, 107(497), 223-232.    Shen, X., Pan, W., Zhu, Y., & Zhou, H. (2013).   constrained regularized high-dimensional regression.   Annals Institute Statistical Mathematics, 65(5), 807-832.    Li, C., Shen, X., & Pan, W. (2021).   Inference Large Directed Graphical Model Interventions.   arXiv preprint arXiv:2110.03805.    Yang, Y., & Zou, H. (2014).   coordinate majorization descent algorithm l1 penalized learning.   Journal Statistical Computation Simulation, 84(1), 84-95.    Two R package Github: ncvreg glmnet.","code":""},{"path":[]},{"path":"https://yuyangyy.com/glmtlp/reference/glmtlp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"glmtlp: A package for fitting a GLM with l0, l1, and tlp regularization. — glmtlp","text":"Maintainer: Yu Yang yuyang.stat@gmail.com (ORCID) [copyright holder] Authors: Chunlin Li chunlin@iastate.edu (ORCID) [copyright holder] Chong Wu (ORCID) [copyright holder] contributors: Xiaotong Shen [thesis advisor, copyright holder] Wei Pan [thesis advisor, copyright holder] Chunlin Li, Yu Yang, Chong Wu    Maintainer: Yu Yang yang6367@umn.edu","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/glmtlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"glmtlp: A package for fitting a GLM with l0, l1, and tlp regularization. — glmtlp","text":"","code":"# Gaussian X <- matrix(rnorm(100 * 20), 100, 20) y <- rnorm(100) fit1 <- glmtlp(X, y, family = \"gaussian\", penalty = \"l0\") fit2 <- glmtlp(X, y, family = \"gaussian\", penalty = \"l1\") fit3 <- glmtlp(X, y, family = \"gaussian\", penalty = \"tlp\")  # Binomial  X <- matrix(rnorm(100 * 20), 100, 20) y <- sample(c(0, 1), 100, replace = TRUE) fit <- glmtlp(X, y, family = \"binomial\", penalty = \"l1\")"},{"path":"https://yuyangyy.com/glmtlp/reference/plot.cv.glmtlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Method for a ","title":"Plot Method for a ","text":"Plots cross-validation curve, upper lower standard deviation   curves, function lambda kappa values.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/plot.cv.glmtlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Method for a ","text":"","code":"# S3 method for class 'cv.glmtlp' plot(x, vertical.line = TRUE, ...)"},{"path":"https://yuyangyy.com/glmtlp/reference/plot.cv.glmtlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Method for a ","text":"x Fitted cv.glmtlp object vertical.line Logical. Whether include vertical line indicating position index gives smallest CV error. ... Additional arguments.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/plot.cv.glmtlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Method for a ","text":"generated plot ggplot object, therefore, users able   customize plots following ggplot2 syntax.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/plot.cv.glmtlp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot Method for a ","text":"Shen, X., Pan, W., & Zhu, Y. (2012).   Likelihood-based selection sharp parameter estimation.   Journal American Statistical Association, 107(497), 223-232.    Shen, X., Pan, W., Zhu, Y., & Zhou, H. (2013).   constrained regularized high-dimensional regression.   Annals Institute Statistical Mathematics, 65(5), 807-832.    Li, C., Shen, X., & Pan, W. (2021).   Inference Large Directed Graphical Model Interventions.   arXiv preprint arXiv:2110.03805.    Yang, Y., & Zou, H. (2014).   coordinate majorization descent algorithm l1 penalized learning.   Journal Statistical Computation Simulation, 84(1), 84-95.    Two R package Github: ncvreg glmnet.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/plot.cv.glmtlp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot Method for a ","text":"Chunlin Li, Yu Yang, Chong Wu    Maintainer: Yu Yang yang6367@umn.edu","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/plot.cv.glmtlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Method for a ","text":"","code":"X <- matrix(rnorm(100 * 20), 100, 20) y <- rnorm(100) cv.fit <- cv.glmtlp(X, y, family = \"gaussian\", penalty = \"tlp\") plot(cv.fit)  plot(cv.fit, vertical.line = FALSE)  cv.fit2 <- cv.glmtlp(X, y, family = \"gaussian\", penalty = \"l0\") plot(cv.fit2)  plot(cv.fit2, vertical.line = FALSE)   data(\"gau_data\") cv.fit <- cv.glmtlp(gau_data$X, gau_data$y, family = \"gaussian\", penalty = \"tlp\") plot(cv.fit)   data(\"bin_data\") cv.fit <- cv.glmtlp(bin_data$X, bin_data$y, family = \"binomial\", penalty = \"l1\") plot(cv.fit)"},{"path":"https://yuyangyy.com/glmtlp/reference/plot.glmtlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Method for a ","title":"Plot Method for a ","text":"Generates solution path plot fitted \"glmtlp\" object.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/plot.glmtlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Method for a ","text":"","code":"# S3 method for class 'glmtlp' plot(   x,   xvar = c(\"lambda\", \"kappa\", \"deviance\", \"l1_norm\", \"log_lambda\"),   xlab = iname,   ylab = \"Coefficients\",   title = \"Solution Path\",   label = FALSE,   label.size = 3,   ... )"},{"path":"https://yuyangyy.com/glmtlp/reference/plot.glmtlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Method for a ","text":"x Fitted glmtlp object. xvar x-axis variable plot , including \"lambda\", \"kappa\", \"deviance\", \"l1_norm\", \"log_lambda\". xlab x-axis label plot, default \"Lambda\", \"Kappa\", \"Fraction Explained Deviance\", \"L1 Norm\", \"Log Lambda\". ylab y-axis label plot, default \"Coefficients\". title main title plot, default \"Solution Path\". label Logical, whether attach labels non-zero coefficients, default FALSE. label.size text size labels, default 3. ... Additional arguments.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/plot.glmtlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Method for a ","text":"ggplot object.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/plot.glmtlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Method for a ","text":"generated plot ggplot object, therefore, users able   customize plots following ggplot2 syntax.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/plot.glmtlp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Plot Method for a ","text":"Shen, X., Pan, W., & Zhu, Y. (2012).   Likelihood-based selection sharp parameter estimation.   Journal American Statistical Association, 107(497), 223-232.    Shen, X., Pan, W., Zhu, Y., & Zhou, H. (2013).   constrained regularized high-dimensional regression.   Annals Institute Statistical Mathematics, 65(5), 807-832.    Li, C., Shen, X., & Pan, W. (2021).   Inference Large Directed Graphical Model Interventions.   arXiv preprint arXiv:2110.03805.    Yang, Y., & Zou, H. (2014).   coordinate majorization descent algorithm l1 penalized learning.   Journal Statistical Computation Simulation, 84(1), 84-95.    Two R package Github: ncvreg glmnet.","code":""},{"path":[]},{"path":"https://yuyangyy.com/glmtlp/reference/plot.glmtlp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Plot Method for a ","text":"Chunlin Li, Yu Yang, Chong Wu    Maintainer: Yu Yang yang6367@umn.edu","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/plot.glmtlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Method for a ","text":"","code":"X <- matrix(rnorm(100 * 20), 100, 20) y <- rnorm(100) fit <- glmtlp(X, y, family = \"gaussian\", penalty = \"l1\") plot(fit, xvar = \"lambda\")  plot(fit, xvar = \"log_lambda\")  plot(fit, xvar = \"l1_norm\")  plot(fit, xvar = \"log_lambda\", label = TRUE)  fit2 <- glmtlp(X, y, family = \"gaussian\", penalty = \"l0\") plot(fit2, xvar = \"kappa\", label = TRUE)"},{"path":"https://yuyangyy.com/glmtlp/reference/predict.cv.glmtlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for a ","title":"Predict Method for a ","text":"Makes predictions cross-validated glmtlp model, using   stored \"glmtlp\" object, optimal value chosen   lambda.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/predict.cv.glmtlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for a ","text":"","code":"# S3 method for class 'cv.glmtlp' predict(   object,   X,   type = c(\"link\", \"response\", \"class\", \"coefficients\", \"numnzs\", \"varnzs\"),   lambda = NULL,   kappa = NULL,   which = object$idx.min,   ... )  # S3 method for class 'cv.glmtlp' coef(object, lambda = NULL, kappa = NULL, which = object$idx.min, ...)"},{"path":"https://yuyangyy.com/glmtlp/reference/predict.cv.glmtlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for a ","text":"object Fitted \"cv.glmtlp\" object. X X Matrix new values X predictions made. Must matrix. type Type prediction made. \"gaussian\" models, type \"link\" \"response\" equivalent give fitted values. \"binomial\" models, type \"link\" gives linear predictors type \"response\" gives fitted probabilities. Type \"coefficients\" computes coefficients provided values lambda kappa. Note \"binomial\" models, results returned class corresponding second level factor response. Type \"class\" applies \"binomial\" models, gives class label corresponding maximum probability. Type \"numnz\" gives total number non-zero coefficients value lambda kappa. Type \"varnz\" gives list indices nonzero coefficients value lambda kappa. lambda Value penalty parameter lambda predictions made Default NULL. kappa Value penalty parameter kappa predictions made. Default NULL. Index penalty parameter lambda kappa sequence predictions made. Default idx.min stored cv.glmtp object. ... Additional arguments.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/predict.cv.glmtlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for a ","text":"object returned depends type.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/predict.cv.glmtlp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Predict Method for a ","text":"Shen, X., Pan, W., & Zhu, Y. (2012).   Likelihood-based selection sharp parameter estimation.   Journal American Statistical Association, 107(497), 223-232.    Shen, X., Pan, W., Zhu, Y., & Zhou, H. (2013).   constrained regularized high-dimensional regression.   Annals Institute Statistical Mathematics, 65(5), 807-832.    Li, C., Shen, X., & Pan, W. (2021).   Inference Large Directed Graphical Model Interventions.   arXiv preprint arXiv:2110.03805.    Yang, Y., & Zou, H. (2014).   coordinate majorization descent algorithm l1 penalized learning.   Journal Statistical Computation Simulation, 84(1), 84-95.    Two R package Github: ncvreg glmnet.","code":""},{"path":[]},{"path":"https://yuyangyy.com/glmtlp/reference/predict.cv.glmtlp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict Method for a ","text":"Chunlin Li, Yu Yang, Chong Wu    Maintainer: Yu Yang yang6367@umn.edu","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/predict.cv.glmtlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for a ","text":"","code":"X <- matrix(rnorm(100 * 20), 100, 20) y <- rnorm(100) cv.fit <- cv.glmtlp(X, y, family = \"gaussian\", penalty = \"l1\") predict(cv.fit, X = X[1:5, ]) #> [1] 0.04564887 0.04564887 0.04564887 0.04564887 0.04564887 coef(cv.fit) #>  intercept         V1         V2         V3         V4         V5         V6  #> 0.04564887 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000  #>         V7         V8         V9        V10        V11        V12        V13  #> 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000  #>        V14        V15        V16        V17        V18        V19        V20  #> 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000  predict(cv.fit, X = X[1:5, ], lambda = 0.1) #> [1] -0.06388020 -0.09547175  0.26124340 -0.20715326  0.18311460"},{"path":"https://yuyangyy.com/glmtlp/reference/predict.glmtlp.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Method for a ","title":"Predict Method for a ","text":"Predicts fitted values, logits, coefficients fitted   glmtlp object.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/predict.glmtlp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Method for a ","text":"","code":"# S3 method for class 'glmtlp' predict(   object,   X,   type = c(\"link\", \"response\", \"class\", \"coefficients\", \"numnz\", \"varnz\"),   lambda = NULL,   kappa = NULL,   which = 1:(ifelse(object$penalty == \"l0\", length(object$kappa), length(object$lambda))),   ... )  # S3 method for class 'glmtlp' coef(   object,   lambda = NULL,   kappa = NULL,   which = 1:(ifelse(object$penalty == \"l0\", length(object$kappa), length(object$lambda))),   drop = TRUE,   ... )"},{"path":"https://yuyangyy.com/glmtlp/reference/predict.glmtlp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Method for a ","text":"object Fitted glmtlp model object. X Matrix new values X predictions made. Must matrix. argument used type=c(\"coefficients\",\"numnz\", \"varnz\"). type Type prediction made. \"gaussian\" models, type \"link\" \"response\" equivalent give fitted values. \"binomial\" models, type \"link\" gives linear predictors type \"response\" gives fitted probabilities. Type \"coefficients\" computes coefficients provided values lambda kappa. Note \"binomial\" models, results returned class corresponding second level factor response. Type \"class\" applies \"binomial\" models, gives class label corresponding maximum probability. Type \"numnz\" gives total number non-zero coefficients value lambda kappa. Type \"varnz\" gives list indices nonzero coefficients value lambda kappa. lambda Value penalty parameter lambda predictions made Default NULL. kappa Value penalty parameter kappa predictions made. Default NULL. Index penalty parameter lambda kappa sequence predictions made. Default indices entire penalty parameter sequence. ... Additional arguments. drop Whether keep dimension length 1.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/predict.glmtlp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Method for a ","text":"object returned depends type.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/predict.glmtlp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Method for a ","text":"coef(...) equivalent predict(type=\"coefficients\",...)","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/predict.glmtlp.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Predict Method for a ","text":"Shen, X., Pan, W., & Zhu, Y. (2012).   Likelihood-based selection sharp parameter estimation.   Journal American Statistical Association, 107(497), 223-232.    Shen, X., Pan, W., Zhu, Y., & Zhou, H. (2013).   constrained regularized high-dimensional regression.   Annals Institute Statistical Mathematics, 65(5), 807-832.    Li, C., Shen, X., & Pan, W. (2021).   Inference Large Directed Graphical Model Interventions.   arXiv preprint arXiv:2110.03805.    Yang, Y., & Zou, H. (2014).   coordinate majorization descent algorithm l1 penalized learning.   Journal Statistical Computation Simulation, 84(1), 84-95.    Two R package Github: ncvreg glmnet.","code":""},{"path":[]},{"path":"https://yuyangyy.com/glmtlp/reference/predict.glmtlp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Predict Method for a ","text":"Chunlin Li, Yu Yang, Chong Wu    Maintainer: Yu Yang yang6367@umn.edu","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/predict.glmtlp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Method for a ","text":"","code":"# Gaussian X <- matrix(rnorm(100 * 20), 100, 20) y <- rnorm(100) fit <- glmtlp(X, y, family = \"gaussian\", penalty = \"l1\") predict(fit, X = X[1:5, ]) #>        0.17208   0.16048   0.14967   0.13958    0.13017    0.12140    0.11322 #> [1,] 0.1207169 0.1319017 0.1465765 0.1603520 0.18252169 0.19475807 0.21225686 #> [2,] 0.1207169 0.1076612 0.1044476 0.1014805 0.08936735 0.07755418 0.06620352 #> [3,] 0.1207169 0.1413063 0.1656688 0.1884552 0.20196349 0.21862114 0.26269766 #> [4,] 0.1207169 0.1421715 0.1613801 0.1793691 0.20609095 0.22933969 0.24976323 #> [5,] 0.1207169 0.1300299 0.1418792 0.1529558 0.15268700 0.14744539 0.15063428 #>         0.10559    0.09847    0.09183    0.08564    0.07987    0.07449 #> [1,] 0.22479617 0.22637330 0.22749405 0.22540928 0.21720349 0.20620084 #> [2,] 0.05572498 0.04568779 0.03914011 0.03186757 0.02412033 0.01438693 #> [3,] 0.30778178 0.34655942 0.38059548 0.40514770 0.41794630 0.42417699 #> [4,] 0.27075312 0.29352343 0.31709779 0.34197466 0.36154212 0.37853943 #> [5,] 0.15222676 0.14820150 0.14594648 0.15060056 0.15620032 0.16521477 #>          0.06947      0.06479     0.06042    0.05635     0.05255     0.04901 #> [1,] 0.198342674  0.190485358  0.18292713  0.1758018  0.16802143  0.16095440 #> [2,] 0.004961516 -0.003861248 -0.01215077 -0.0227093 -0.02684029 -0.02975411 #> [3,] 0.428643769  0.432723629  0.43651365  0.4496747  0.45952602  0.46741131 #> [4,] 0.396263601  0.412726057  0.42811035  0.4449499  0.46162493  0.47340421 #> [5,] 0.168303818  0.171610784  0.17467838  0.1749932  0.17836337  0.18227295 #>          0.04571     0.04263     0.03975     0.03707     0.03457     0.03224 #> [1,]  0.15405945  0.14776749  0.14175716  0.13614769  0.13079517  0.12538494 #> [2,] -0.03263259 -0.03524822 -0.03775293 -0.04009525 -0.04231938 -0.04418203 #> [3,]  0.47492286  0.48183795  0.48836955  0.49447017  0.50027633  0.50164964 #> [4,]  0.48455825  0.49499694  0.50470903  0.51375960  0.52118995  0.52639822 #> [5,]  0.18611882  0.18952629  0.19287374  0.19600971  0.19902480  0.20129283 #>          0.03007     0.02804    0.02615     0.02439     0.02275     0.02121 #> [1,]  0.12000720  0.11497824  0.1102874  0.10606508  0.10198185  0.09816722 #> [2,] -0.04608474 -0.04786285 -0.0495210 -0.05104691 -0.05249264 -0.05383947 #> [3,]  0.50279419  0.50385484  0.5048429  0.50585234  0.50671433  0.50751100 #> [4,]  0.53139965  0.53607539  0.5404372  0.54450418  0.54829629  0.55183395 #> [5,]  0.20352010  0.20560062  0.2075404  0.20925337  0.21094368  0.21252377 #>          0.01978     0.01845     0.01721     0.01605     0.01497     0.01396 #> [1,]  0.09460932  0.09129118  0.08819667  0.08546789  0.08277710  0.08025727 #> [2,] -0.05509492 -0.05626561 -0.05735738 -0.05835120 -0.05930383 -0.06019179 #> [3,]  0.50825274  0.50894424  0.50958907  0.51027522  0.51084123  0.51136038 #> [4,]  0.55513373  0.55821126  0.56108141  0.56375459  0.56624979  0.56857732 #> [5,]  0.21399695  0.21537068  0.21665178  0.21775251  0.21886648  0.21991235 #>          0.01302     0.01214     0.01132     0.01056     0.00985     0.00918 #> [1,]  0.07790650  0.07571408  0.07366941  0.07176255  0.07013508  0.06848361 #> [2,] -0.06101923 -0.06179069 -0.06251010 -0.06318101 -0.06378034 -0.06436564 #> [3,]  0.51184291  0.51229251  0.51271171  0.51310262  0.51354242  0.51389080 #> [4,]  0.57074855  0.57277365  0.57466232  0.57642372  0.57805916  0.57959086 #> [5,]  0.22088766  0.22179703  0.22264503  0.22343587  0.22408833  0.22477080 #>          0.00856     0.00799     0.00745     0.00695     0.00648     0.00604 #> [1,]  0.06692819  0.06547591  0.06412130  0.06285795  0.06183488  0.06075432 #> [2,] -0.06491250 -0.06542193 -0.06589674 -0.06633945 -0.06672207 -0.06710678 #> [3,]  0.51420527  0.51449642  0.51476733  0.51501979  0.51532504  0.51555879 #> [4,]  0.58101926  0.58235182  0.58359482  0.58475415  0.58582162  0.58683047 #> [5,]  0.22541791  0.22602218  0.22658557  0.22711089  0.22751899  0.22796286 #>          0.00563     0.00526     0.00490     0.00457     0.00426     0.00398 #> [1,]  0.05945329  0.05863153  0.05771591  0.05683431  0.05553185  0.05416548 #> [2,] -0.06743013 -0.06756315 -0.06773295 -0.06789897 -0.06831074 -0.06901595 #> [3,]  0.51544303  0.51543871  0.51540510  0.51536306  0.51511387  0.51461683 #> [4,]  0.58777506  0.58860480  0.58937359  0.59008663  0.59090326  0.59192988 #> [5,]  0.22870619  0.22933880  0.23001249  0.23066408  0.23143922  0.23196647 #>          0.00371     0.00346     0.00322     0.00301     0.00280     0.00262 #> [1,]  0.05277150  0.05144875  0.05038551  0.04926573  0.04818651  0.04717082 #> [2,] -0.06971903 -0.07038142 -0.07093158 -0.07149450 -0.07203284 -0.07253842 #> [3,]  0.51409767  0.51359238  0.51321014  0.51279081  0.51237698  0.51198281 #> [4,]  0.59292189  0.59385315  0.59467473  0.59548132  0.59624014  0.59695007 #> [5,]  0.23245239  0.23290431  0.23328244  0.23366822  0.23403511  0.23437770 #>          0.00244     0.00227     0.00212     0.00198     0.00185     0.00172 #> [1,]  0.04622100  0.04533439  0.04450727  0.04391942  0.04306805  0.04252640 #> [2,] -0.07301075 -0.07345136 -0.07386223 -0.07417105 -0.07458300 -0.07486322 #> [3,]  0.51161188  0.51126447  0.51093975  0.51073024  0.51038981  0.51018985 #> [4,]  0.59761333  0.59823255  0.59881041  0.59928256  0.59984089  0.60025766 #> [5,]  0.23469644  0.23499300  0.23526910  0.23548053  0.23575778  0.23594882 #>          0.00160     0.00150     0.00140     0.00130     0.00121     0.00113 #> [1,]  0.04194723  0.04137271  0.04064321  0.04017255  0.03968239  0.03920126 #> [2,] -0.07515658 -0.07544330 -0.07582330 -0.07616303 -0.07649850 -0.07681741 #> [3,]  0.50997368  0.50975411  0.50950512  0.50947028  0.50943038  0.50939073 #> [4,]  0.60068478  0.60109197  0.60151169  0.60176601  0.60200444  0.60221600 #> [5,]  0.23614868  0.23634365  0.23645189  0.23637716  0.23633917  0.23631066 #>          0.00106     0.00098     0.00092     0.00086     0.00080     0.00074 #> [1,]  0.03874247  0.03849728  0.03799166  0.03774706  0.03727330  0.03705201 #> [2,] -0.07711740 -0.07728903 -0.07761975 -0.07778555 -0.07808923 -0.07823704 #> [3,]  0.50935370  0.50934979  0.50931001  0.50930038  0.50925828  0.50924803 #> [4,]  0.60240506  0.60253918  0.60274560  0.60286256  0.60303540  0.60313355 #> [5,]  0.23628649  0.23626156  0.23623836  0.23621936  0.23620304  0.23618757 #>          0.00069     0.00065     0.00060     0.00056    0.00053     0.00049 #> [1,]  0.03681130  0.03656013  0.03630728  0.03605913  0.0358197  0.03559127 #> [2,] -0.07840094 -0.07857031 -0.07873841 -0.07890134 -0.0790570 -0.07920439 #> [3,]  0.50923934  0.50922741  0.50921239  0.50919543  0.5091776  0.50915967 #> [4,]  0.60324925  0.60336534  0.60347535  0.60357720  0.6036706  0.60375612 #> [5,]  0.23616912  0.23615234  0.23613798  0.23612575  0.2361152  0.23610594 #>          0.00046     0.00043     0.00040     0.00037     0.00035     0.00032 #> [1,]  0.03537495  0.03517111  0.03497966  0.03480024  0.03463235  0.03447541 #> [2,] -0.07934318 -0.07947340 -0.07959531 -0.07970925 -0.07981565 -0.07991495 #> [3,]  0.50914212  0.50912522  0.50910912  0.50909388  0.50907951  0.50906601 #> [4,]  0.60383436  0.60390610  0.60397202  0.60403275  0.60408880  0.60414065 #> [5,]  0.23609762  0.23609004  0.23608304  0.23607653  0.23607043  0.23606469 #>          0.00030     0.00028     0.00026     0.00024     0.00023     0.00021 #> [1,]  0.03432880  0.03419193  0.03406417  0.03394495  0.03383372  0.03372996 #> [2,] -0.08000758 -0.08009397 -0.08017453 -0.08024964 -0.08031968 -0.08038497 #> [3,]  0.50905334  0.50904146  0.50903034  0.50901993  0.50901019  0.50900108 #> [4,]  0.60418868  0.60423324  0.60427462  0.60431309  0.60434887  0.60438217 #> [5,]  0.23605930  0.23605421  0.23604942  0.23604490  0.23604065  0.23603665 #>          0.00020     0.00018     0.00017 #> [1,]  0.03363317  0.03354289  0.03345869 #> [2,] -0.08044586 -0.08050262 -0.08055555 #> [3,]  0.50899256  0.50898460  0.50897716 #> [4,]  0.60441319  0.60444208  0.60446900 #> [5,]  0.23603289  0.23602936  0.23602605 coef(fit) #>             0.17208      0.16048     0.14967       0.13958     0.13017 #> intercept 0.1207169  0.121950801  0.12357204  1.251088e-01  0.12738388 #> V1        0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V2        0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V3        0.0000000  0.000000000  0.00000000  2.728706e-05  0.01022468 #> V4        0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V5        0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V6        0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V7        0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V8        0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V9        0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V10       0.0000000 -0.009264333 -0.02260555 -3.509981e-02 -0.04864402 #> V11       0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V12       0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V13       0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V14       0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V15       0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V16       0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V17       0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V18       0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #> V19       0.0000000 -0.013803115 -0.02669708 -3.874678e-02 -0.05041492 #> V20       0.0000000  0.000000000  0.00000000  0.000000e+00  0.00000000 #>                0.12140      0.11322      0.10559     0.09847      0.09183 #> intercept  0.129039788  0.130733366  0.132236280  0.13316245  0.133345746 #> V1         0.000000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V2         0.000000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V3         0.017803606  0.024790224  0.031865945  0.03897277  0.045612131 #> V4         0.000000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V5         0.000000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V6         0.000000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V7         0.000000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V8         0.000000000  0.008489263  0.017412040  0.02451683  0.030951454 #> V9         0.000000000  0.000000000  0.000000000  0.00000000 -0.002709557 #> V10       -0.061003969 -0.073001617 -0.084471830 -0.09495931 -0.104899575 #> V11        0.000000000  0.000000000  0.000000000  0.00000000 -0.002034004 #> V12        0.000000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V13        0.000000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V14        0.000000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V15        0.000000000  0.000000000  0.002813425  0.01039236  0.018166167 #> V16        0.000000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V17       -0.008413854 -0.017745109 -0.026593568 -0.03489632 -0.042750578 #> V18        0.000000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V19       -0.062233640 -0.073729565 -0.084677379 -0.09509313 -0.104791879 #> V20        0.000000000  0.000000000  0.000000000  0.00000000  0.000000000 #>                0.08564      0.07987       0.07449      0.06947      0.06479 #> intercept  0.132324633  0.130593205  0.1284631727  0.126376096  0.124357364 #> V1         0.000000000 -0.005140795 -0.0128641697 -0.019462694 -0.025756799 #> V2         0.000000000  0.000000000  0.0000000000  0.000000000  0.000000000 #> V3         0.051020726  0.055820734  0.0599690923  0.063947777  0.067607644 #> V4         0.000000000  0.000000000  0.0000000000  0.000000000  0.000000000 #> V5         0.000000000  0.000000000  0.0003466662  0.002566244  0.004511952 #> V6         0.000000000  0.000000000  0.0000000000  0.000000000  0.000000000 #> V7         0.000000000  0.002261669  0.0079546263  0.012933801  0.017640611 #> V8         0.036115145  0.041471927  0.0467146677  0.051164504  0.055346639 #> V9        -0.008257326 -0.013308761 -0.0178720909 -0.022212171 -0.026323373 #> V10       -0.114040948 -0.122386012 -0.1300852556 -0.136774723 -0.143093344 #> V11       -0.009605408 -0.016675210 -0.0231660384 -0.029158270 -0.034863677 #> V12        0.000000000  0.000000000  0.0000000000  0.000000000  0.000000000 #> V13        0.000000000  0.000000000  0.0000000000  0.000000000  0.000000000 #> V14        0.000000000  0.000000000  0.0000000000  0.000000000  0.000000000 #> V15        0.026740749  0.034478350  0.0412377762  0.047661471  0.053740361 #> V16        0.000000000  0.000000000  0.0000000000  0.000000000  0.000000000 #> V17       -0.050869148 -0.058980939 -0.0669552061 -0.074370296 -0.081398420 #> V18        0.000000000  0.000000000  0.0000000000  0.000000000  0.000000000 #> V19       -0.113454861 -0.121909356 -0.1296125901 -0.136503906 -0.143018523 #> V20        0.000000000  0.000000000  0.0000000000  0.000000000  0.000000000 #>                 0.06042       0.05635       0.05255      0.04901      0.04571 #> intercept  1.224369e-01  0.1210763342  0.1197852740  0.118946056  0.118146409 #> V1        -3.167069e-02 -0.0369639974 -0.0418026461 -0.047017326 -0.051877473 #> V2         0.000000e+00  0.0000000000  0.0000000000  0.000000000  0.000000000 #> V3         7.099789e-02  0.0740953516  0.0763028254  0.078353242  0.080205686 #> V4         0.000000e+00  0.0000000000 -0.0008685562 -0.004581759 -0.007992910 #> V5         6.304680e-03  0.0073701877  0.0090508823  0.010379408  0.011566243 #> V6         0.000000e+00 -0.0001256476 -0.0049091069 -0.008498118 -0.011885950 #> V7         2.204433e-02  0.0262988256  0.0299721732  0.033487114  0.036790486 #> V8         5.924715e-02  0.0624917588  0.0646605499  0.067458260  0.070005062 #> V9        -3.018871e-02 -0.0326601727 -0.0347076183 -0.035935480 -0.037078816 #> V10       -1.489990e-01 -0.1558799898 -0.1629995629 -0.169944002 -0.176486505 #> V11       -4.023682e-02 -0.0462020001 -0.0516219647 -0.056572738 -0.061308173 #> V12        0.000000e+00 -0.0017158345 -0.0044491317 -0.007115411 -0.009647606 #> V13        0.000000e+00  0.0000000000  0.0000000000  0.000000000  0.000000000 #> V14        0.000000e+00  0.0000000000  0.0000000000  0.000000000  0.000000000 #> V15        5.945432e-02  0.0652395926  0.0712484055  0.076415209  0.081316078 #> V16        0.000000e+00  0.0000000000  0.0000000000  0.000000000  0.000000000 #> V17       -8.800921e-02 -0.0945358799 -0.1006866984 -0.106041731 -0.111163003 #> V18       -4.569895e-05 -0.0049618837 -0.0089231012 -0.012593822 -0.016103880 #> V19       -1.491253e-01 -0.1554298686 -0.1610156941 -0.166092327 -0.170898293 #> V20        0.000000e+00  0.0000000000  0.0000000000  0.000000000  0.000000000 #>               0.04263     0.03975     0.03707       0.03457      0.03224 #> intercept  0.11739653  0.11669883  0.11604978  0.1154270846  0.114743242 #> V1        -0.05637343 -0.06059858 -0.06454127 -0.0682839660 -0.071568584 #> V2         0.00000000  0.00000000  0.00000000 -0.0009238186 -0.004515547 #> V3         0.08195854  0.08356666  0.08506468  0.0864535787  0.088371086 #> V4        -0.01114612 -0.01410998 -0.01687698 -0.0194760449 -0.021607992 #> V5         0.01271843  0.01375181  0.01471178  0.0154546902  0.016217263 #> V6        -0.01505892 -0.01800962 -0.02075837 -0.0233079830 -0.026081307 #> V7         0.03985299  0.04272731  0.04540900  0.0480380209  0.051132334 #> V8         0.07237909  0.07458939  0.07665193  0.0785223228  0.079544546 #> V9        -0.03817703 -0.03917319 -0.04009858 -0.0409771921 -0.042473607 #> V10       -0.18253161 -0.18822413 -0.19353579 -0.1985806972 -0.202602410 #> V11       -0.06566165 -0.06978565 -0.07363471 -0.0773013762 -0.080121521 #> V12       -0.01197389 -0.01417773 -0.01623546 -0.0183627389 -0.020810192 #> V13        0.00000000  0.00000000  0.00000000  0.0000000000  0.000000000 #> V14        0.00000000  0.00000000  0.00000000 -0.0001931562 -0.002650609 #> V15        0.08585616  0.09012404  0.09410403  0.0979428454  0.102156740 #> V16        0.00000000  0.00000000  0.00000000  0.0000000000  0.000000000 #> V17       -0.11588808 -0.12034729 -0.12450828 -0.1283814500 -0.131077331 #> V18       -0.01932841 -0.02238334 -0.02523556 -0.0279870846 -0.030047622 #> V19       -0.17534182 -0.17952265 -0.18342399 -0.1871197930 -0.190141563 #> V20        0.00000000  0.00000000  0.00000000  0.0000000000  0.000000000 #>                0.03007      0.02804     0.02615     0.02439     0.02275 #> intercept  0.114087252  0.113473990  0.11290183  0.11238152  0.11188449 #> V1        -0.074634643 -0.077491044 -0.08015454 -0.08262207 -0.08494178 #> V2        -0.007860949 -0.010979804 -0.01388847 -0.01655440 -0.01908165 #> V3         0.090110241  0.091728095  0.09323668  0.09464541  0.09595740 #> V4        -0.023601202 -0.025457617 -0.02718852 -0.02879958 -0.03030737 #> V5         0.016932336  0.017601795  0.01822656  0.01883688  0.01937969 #> V6        -0.028710546 -0.031166786 -0.03345803 -0.03558030 -0.03757245 #> V7         0.054058116  0.056788597  0.05933515  0.06167093  0.06388528 #> V8         0.080417731  0.081226464  0.08198015  0.08273090  0.08338958 #> V9        -0.043854417 -0.045143119 -0.04634528 -0.04745830 -0.04850189 #> V10       -0.206398715 -0.209939267 -0.21324102 -0.21629064 -0.21916355 #> V11       -0.082866082 -0.085430572 -0.08782243 -0.09000827 -0.09208912 #> V12       -0.023115078 -0.025265446 -0.02727083 -0.02909469 -0.03083798 #> V13        0.000000000  0.000000000  0.00000000  0.00000000  0.00000000 #> V14       -0.004974462 -0.007143267 -0.00916607 -0.01103006 -0.01278953 #> V15        0.106181540  0.109939561  0.11344469  0.11665030  0.11969812 #> V16        0.000000000  0.000000000  0.00000000  0.00000000  0.00000000 #> V17       -0.133692035 -0.136135589 -0.13841472 -0.14052425 -0.14250836 #> V18       -0.032020857 -0.033862294 -0.03557952 -0.03715422 -0.03864830 #> V19       -0.192987181 -0.195640396 -0.19811460 -0.20040324 -0.20255651 #> V20        0.000000000  0.000000000  0.00000000  0.00000000  0.00000000 #>               0.02121     0.01978     0.01845     0.01721     0.01605 #> intercept  0.11141971  0.11098603  0.11058152  0.11020427  0.10986569 #> V1        -0.08710356 -0.08911924 -0.09099899 -0.09275203 -0.09436468 #> V2        -0.02144350 -0.02364685 -0.02570181 -0.02761829 -0.02936071 #> V3         0.09718080  0.09832184  0.09938601  0.10037846  0.10130339 #> V4        -0.03171187 -0.03302129 -0.03424238 -0.03538115 -0.03643651 #> V5         0.01988458  0.02035552  0.02079475  0.02120440  0.02161292 #> V6        -0.03943161 -0.04116582 -0.04278322 -0.04429164 -0.04568212 #> V7         0.06595323  0.06788209  0.06968099  0.07135865  0.07288460 #> V8         0.08399831  0.08456525  0.08509386  0.08558681  0.08608817 #> V9        -0.04947716 -0.05038717 -0.05123594 -0.05202754 -0.05275886 #> V10       -0.22184367 -0.22434311 -0.22667406 -0.22884790 -0.23084214 #> V11       -0.09403213 -0.09584428 -0.09753430 -0.09911041 -0.10053494 #> V12       -0.03246745 -0.03398740 -0.03540494 -0.03672694 -0.03791555 #> V13        0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 #> V14       -0.01443162 -0.01596320 -0.01739159 -0.01872372 -0.01994206 #> V15        0.12254496  0.12520048  0.12767711  0.12998683  0.13207681 #> V16        0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 #> V17       -0.14435827 -0.14608328 -0.14769199 -0.14919226 -0.15057222 #> V18       -0.04004299 -0.04134366 -0.04255664 -0.04368786 -0.04471580 #> V19       -0.20456475 -0.20643749 -0.20818398 -0.20981274 -0.21131031 #> V20        0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 #>               0.01497     0.01396     0.01302     0.01214     0.01132 #> intercept  0.10953850  0.10923195  0.10894575  0.10867877  0.10842976 #> V1        -0.09589261 -0.09731653 -0.09864396 -0.09988180 -0.10103617 #> V2        -0.03102361 -0.03258014 -0.03403279 -0.03538774 -0.03665142 #> V3         0.10216695  0.10297185  0.10372258  0.10442275  0.10507574 #> V4        -0.03742947 -0.03835414 -0.03921597 -0.04001958 -0.04076899 #> V5         0.02196963  0.02230008  0.02260820  0.02289560  0.02316365 #> V6        -0.04699333 -0.04821744 -0.04935944 -0.05042460 -0.05141800 #> V7         0.07434203  0.07570499  0.07697657  0.07816252  0.07926855 #> V8         0.08652208  0.08692020  0.08729035  0.08763533  0.08795701 #> V9        -0.05344516 -0.05408707 -0.05468629 -0.05524530 -0.05576666 #> V10       -0.23273356 -0.23449905 -0.23614556 -0.23768108 -0.23911309 #> V11       -0.10190496 -0.10318622 -0.10438141 -0.10549605 -0.10653557 #> V12       -0.03906286 -0.04013767 -0.04114062 -0.04207605 -0.04294844 #> V13        0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 #> V14       -0.02110038 -0.02218225 -0.02319144 -0.02413266 -0.02501046 #> V15        0.13408328  0.13596023  0.13771150  0.13934491  0.14086826 #> V16        0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 #> V17       -0.15187867 -0.15309713 -0.15423321 -0.15529265 -0.15628067 #> V18       -0.04569936 -0.04661873 -0.04747622 -0.04827590 -0.04902166 #> V19       -0.21272794 -0.21405067 -0.21528412 -0.21643439 -0.21750711 #> V20        0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 #>               0.01056     0.00985     0.00918     0.00856     0.00799 #> intercept  0.10819753  0.10799354  0.10779284  0.10760408  0.10742766 #> V1        -0.10211274 -0.10309030 -0.10402914 -0.10490483 -0.10572095 #> V2        -0.03782993 -0.03888725 -0.03990691 -0.04086409 -0.04175820 #> V3         0.10568472  0.10624908  0.10677959  0.10727368  0.10773446 #> V4        -0.04146789 -0.04211008 -0.04272016 -0.04328841 -0.04381781 #> V5         0.02341364  0.02366984  0.02388916  0.02409050  0.02427797 #> V6        -0.05234447 -0.05319094 -0.05399587 -0.05474792 -0.05544971 #> V7         0.08030004  0.08122567  0.08211961  0.08295797  0.08374061 #> V8         0.08825699  0.08856970  0.08883769  0.08908048  0.08930529 #> V9        -0.05625290 -0.05670036 -0.05712117 -0.05751513 -0.05788319 #> V10       -0.24044859 -0.24165895 -0.24282019 -0.24390570 -0.24491824 #> V11       -0.10750502 -0.10836580 -0.10920631 -0.10999509 -0.11073132 #> V12       -0.04376204 -0.04448078 -0.04518401 -0.04584572 -0.04646384 #> V13        0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 #> V14       -0.02582910 -0.02656818 -0.02727919 -0.02794447 -0.02856525 #> V15        0.14228895  0.14355227  0.14478333  0.14593851  0.14701711 #> V16        0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 #> V17       -0.15720210 -0.15804007 -0.15884237 -0.15959151 -0.16028995 #> V18       -0.04971716 -0.05033993 -0.05094334 -0.05150903 -0.05203692 #> V19       -0.21850753 -0.21941749 -0.22028786 -0.22110101 -0.22185932 #> V20        0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 #>               0.00745     0.00695     0.00648     0.00604       0.00563 #> intercept  0.10726302  0.10710944  0.10697934  0.10684776  0.1066989296 #> V1        -0.10648187 -0.10719144 -0.10782034 -0.10843838 -0.1090575484 #> V2        -0.04259241 -0.04337049 -0.04405238 -0.04472141 -0.0454243364 #> V3         0.10816423  0.10856505  0.10893145  0.10928129  0.1096028417 #> V4        -0.04431132 -0.04477150 -0.04518724 -0.04558908 -0.0459821329 #> V5         0.02445282  0.02461590  0.02478856  0.02493453  0.0250352687 #> V6        -0.05610436 -0.05671495 -0.05726334 -0.05779293 -0.0583104403 #> V7         0.08447066  0.08515154  0.08574863  0.08633520  0.0869527666 #> V8         0.08951455  0.08970960  0.08991827  0.09009779  0.0902045265 #> V9        -0.05822667 -0.05854708 -0.05883939 -0.05911599 -0.0593861202 #> V10       -0.24586251 -0.24674313 -0.24752350 -0.24828713 -0.2490661957 #> V11       -0.11141799 -0.11205839 -0.11261033 -0.11316156 -0.1137447080 #> V12       -0.04704047 -0.04757827 -0.04804013 -0.04850055 -0.0490011218 #> V13        0.00000000  0.00000000  0.00000000  0.00000000  0.0000000000 #> V14       -0.02914428 -0.02968430 -0.03016066 -0.03062793 -0.0311077515 #> V15        0.14802328  0.14896172  0.14977193  0.15057962  0.1514415937 #> V16        0.00000000  0.00000000  0.00000000  0.00000000  0.0001693112 #> V17       -0.16094122 -0.16154855 -0.16208928 -0.16261708 -0.1631520469 #> V18       -0.05252923 -0.05298834 -0.05338933 -0.05378526 -0.0542058000 #> V19       -0.22256646 -0.22322591 -0.22381379 -0.22438622 -0.2249672113 #> V20        0.00000000  0.00000000  0.00000000  0.00000000  0.0000000000 #>                 0.00526       0.00490      0.00457       0.00426      0.00398 #> intercept  0.1065879456  0.1064753187  0.106369104  0.1062654166  0.106226056 #> V1        -0.1095210190 -0.1099955157 -0.110443154 -0.1108902003 -0.111221278 #> V2        -0.0459702147 -0.0465100489 -0.047019474 -0.0475597113 -0.048080554 #> V3         0.1098173954  0.1100248565  0.110217698  0.1103902274  0.110582403 #> V4        -0.0463196336 -0.0466560601 -0.046972261 -0.0472831814 -0.047527455 #> V5         0.0252055462  0.0253409933  0.025460491  0.0255490568  0.025723514 #> V6        -0.0586858478 -0.0590541418 -0.059399069 -0.0597270561 -0.059994001 #> V7         0.0874527519  0.0879524964  0.088424848  0.0889060018  0.089191660 #> V8         0.0903712203  0.0905168271  0.090646265  0.0907315932  0.090876179 #> V9        -0.0596136839 -0.0598222028 -0.060015826 -0.0602010494 -0.060396799 #> V10       -0.2496972873 -0.2503280324 -0.250922079 -0.2515240987 -0.251980268 #> V11       -0.1141212958 -0.1145198098 -0.114901193 -0.1153074896 -0.115608104 #> V12       -0.0493561487 -0.0497213284 -0.050070306 -0.0504515182 -0.050763220 #> V13        0.0000000000  0.0000000000  0.000000000 -0.0001621252 -0.000660091 #> V14       -0.0314741852 -0.0318392299 -0.032183023 -0.0325353033 -0.032857132 #> V15        0.1520518994  0.1526761198  0.153268066  0.1538892886  0.154339836 #> V16        0.0005610914  0.0009080558  0.001229122  0.0015648392  0.001943072 #> V17       -0.1636191102 -0.1640862764 -0.164525500 -0.1649758919 -0.165348661 #> V18       -0.0545603043 -0.0549155482 -0.055252093 -0.0556044249 -0.055843899 #> V19       -0.2254745457 -0.2259722676 -0.226440028 -0.2269115535 -0.227285827 #> V20        0.0000000000  0.0000000000  0.000000000  0.0000000000  0.000000000 #>                0.00371      0.00346      0.00322      0.00301      0.00280 #> intercept  0.106181416  0.106137867  0.106102143  0.106064714  0.106028753 #> V1        -0.111538861 -0.111831714 -0.112086458 -0.112342417 -0.112581211 #> V2        -0.048625416 -0.049149066 -0.049574875 -0.050015769 -0.050442570 #> V3         0.110765021  0.110935706  0.111091079  0.111241292  0.111380784 #> V4        -0.047762919 -0.047980106 -0.048170273 -0.048358870 -0.048535263 #> V5         0.025889289  0.026043409  0.026184323  0.026319139  0.026444535 #> V6        -0.060247445 -0.060483901 -0.060698849 -0.060906818 -0.061100189 #> V7         0.089468305  0.089727502  0.089955880  0.090181123  0.090392555 #> V8         0.090990826  0.091087374  0.091189767  0.091280971  0.091359040 #> V9        -0.060590189 -0.060774718 -0.060936551 -0.061095531 -0.061246316 #> V10       -0.252427233 -0.252845127 -0.253203059 -0.253564404 -0.253905116 #> V11       -0.115909169 -0.116194232 -0.116431076 -0.116673798 -0.116906052 #> V12       -0.051091912 -0.051409094 -0.051665256 -0.051930468 -0.052188476 #> V13       -0.001158227 -0.001631361 -0.002016996 -0.002416372 -0.002801387 #> V14       -0.033176174 -0.033477157 -0.033730305 -0.033988124 -0.034233630 #> V15        0.154808068  0.155254870  0.155624039  0.156004252  0.156369067 #> V16        0.002317054  0.002670741  0.002963210  0.003262699  0.003550067 #> V17       -0.165712510 -0.166052038 -0.166339424 -0.166631248 -0.166907560 #> V18       -0.056087349 -0.056317646 -0.056510476 -0.056706537 -0.056893798 #> V19       -0.227663141 -0.228018420 -0.228320199 -0.228624553 -0.228913550 #> V20        0.000000000  0.000000000  0.000000000  0.000000000  0.000000000 #>                0.00262      0.00244      0.00227      0.00212      0.00198 #> intercept  0.105994795  0.105962897  0.105933016  0.105905073  0.105885116 #> V1        -0.112802383 -0.113007651 -0.113198566 -0.113376359 -0.113518456 #> V2        -0.050846700 -0.051226098 -0.051580995 -0.051912442 -0.052151927 #> V3         0.111510583  0.111631646  0.111744630  0.111850066  0.111939782 #> V4        -0.048698973 -0.048851015 -0.048992431 -0.049124109 -0.049231652 #> V5         0.026561550  0.026670859  0.026772947  0.026868253  0.026949897 #> V6        -0.061280095 -0.061447830 -0.061604339 -0.061750388 -0.061874714 #> V7         0.090589864  0.090773871  0.090945469  0.091105498  0.091235822 #> V8         0.091428182  0.091490977  0.091548776  0.091602328  0.091661285 #> V9        -0.061388318 -0.061521545 -0.061646236 -0.061762764 -0.061856612 #> V10       -0.254223070 -0.254519421 -0.254795659 -0.255053195 -0.255253937 #> V11       -0.117124224 -0.117328088 -0.117518296 -0.117695694 -0.117828618 #> V12       -0.052433199 -0.052662967 -0.052877825 -0.053078421 -0.053222998 #> V13       -0.003164720 -0.003505130 -0.003823205 -0.004120090 -0.004336118 #> V14       -0.034463952 -0.034679203 -0.034880129 -0.035067599 -0.035210066 #> V15        0.156712624  0.157034274  0.157334770  0.157615252  0.157822510 #> V16        0.003820713  0.004074008  0.004310526  0.004531196  0.004695642 #> V17       -0.167165633 -0.167406110 -0.167630183 -0.167839030 -0.168000016 #> V18       -0.057069649 -0.057233933 -0.057387176 -0.057530067 -0.057638677 #> V19       -0.229184309 -0.229437072 -0.229672812 -0.229892631 -0.230062915 #> V20        0.000000000  0.000000000  0.000000000  0.000000000  0.000000000 #>                0.00185      0.00172      0.00160      0.00150       0.00140 #> intercept  0.105856439  0.105838223  0.105818617  0.105799293  1.057702e-01 #> V1        -0.113697764 -0.113823036 -0.113956374 -0.114084572 -1.142234e-01 #> V2        -0.052484924 -0.052705142 -0.052935762 -0.053162956 -5.344449e-02 #> V3         0.112040770  0.112118692  0.112198555  0.112273818  1.123502e-01 #> V4        -0.049361405 -0.049456266 -0.049554841 -0.049649225 -4.974998e-02 #> V5         0.027040141  0.027110972  0.027183135  0.027250791  2.731903e-02 #> V6        -0.062014451 -0.062122627 -0.062233148 -0.062337394 -6.244333e-02 #> V7         0.091391913  0.091506861  0.091625684  0.091739122  9.186108e-02 #> V8         0.091710381  0.091757502  0.091806034  0.091848890  9.187531e-02 #> V9        -0.061968272 -0.062051638 -0.062136339 -0.062217260 -6.230654e-02 #> V10       -0.255512145 -0.255691002 -0.255879918 -0.256062483 -2.562677e-01 #> V11       -0.118006760 -0.118127449 -0.118253994 -0.118377740 -1.185245e-01 #> V12       -0.053423251 -0.053556973 -0.053695542 -0.053832361 -5.400348e-02 #> V13       -0.004635693 -0.004833726 -0.005041923 -0.005246909 -5.498192e-03 #> V14       -0.035397859 -0.035526282 -0.035661131 -0.035792368 -3.594524e-02 #> V15        0.158105011  0.158293740  0.158492313  0.158686968  1.589212e-01 #> V16        0.004916455  0.005066093  0.005222525  0.005375580  5.558680e-03 #> V17       -0.168209816 -0.168354189 -0.168506273 -0.168653924 -1.688226e-01 #> V18       -0.057781033 -0.057879287 -0.057981672 -0.058081411 -5.819813e-02 #> V19       -0.230281178 -0.230433979 -0.230593318 -0.230747639 -2.309239e-01 #> V20        0.000000000  0.000000000  0.000000000  0.000000000  5.554657e-05 #>                 0.00130       0.00121       0.00113       0.00106       0.00098 #> intercept  0.1057365816  0.1056994903  0.1056622386  0.1056261393  0.1056071239 #> V1        -0.1143248083 -0.1144396777 -0.1145546616 -0.1146651280 -0.1147303615 #> V2        -0.0536197256 -0.0538019900 -0.0539818026 -0.0541546459 -0.0542500915 #> V3         0.1124039126  0.1124579378  0.1125084132  0.1125551411  0.1125884358 #> V4        -0.0498223584 -0.0499007183 -0.0499776334 -0.0500507866 -0.0500970363 #> V5         0.0273665212  0.0274091018  0.0274450736  0.0274759814  0.0275001747 #> V6        -0.0625342592 -0.0626340600 -0.0627316668 -0.0628243429 -0.0628834677 #> V7         0.0919544501  0.0920507650  0.0921425440  0.0922287438  0.0922827223 #> V8         0.0919083756  0.0919457998  0.0919807213  0.0920124435  0.0920390542 #> V9        -0.0623597942 -0.0624099894 -0.0624562351 -0.0624988405 -0.0625292267 #> V10       -0.2564165814 -0.2565793899 -0.2567397526 -0.2568927246 -0.2569814042 #> V11       -0.1186277512 -0.1187381351 -0.1188469405 -0.1189511310 -0.1190095075 #> V12       -0.0541210501 -0.0542478116 -0.0543753261 -0.0544994605 -0.0545668788 #> V13       -0.0056614466 -0.0058301800 -0.0059942235 -0.0061500556 -0.0062356700 #> V14       -0.0360355468 -0.0361244838 -0.0362084391 -0.0362866068 -0.0363342399 #> V15        0.1590711082  0.1592263494  0.1593775647  0.1595213886  0.1596023984 #> V16        0.0056685253  0.0057754754  0.0058760305  0.0059696519  0.0060249770 #> V17       -0.1689229645 -0.1690233832 -0.1691184073 -0.1692067055 -0.1692613479 #> V18       -0.0582631251 -0.0583251132 -0.0583834611 -0.0584379679 -0.0584725053 #> V19       -0.2310469730 -0.2311764632 -0.2313027668 -0.2314230176 -0.2314941601 #> V20        0.0001986344  0.0003435978  0.0004869658  0.0006254223  0.0007045883 #>                 0.00092      0.00086      0.00080      0.00074      0.00069 #> intercept  0.1055674114  0.105548262  0.105510577  0.105492987  0.105474045 #> V1        -0.1148543575 -0.114917263 -0.115030944 -0.115086974 -0.115149591 #> V2        -0.0544370314 -0.054530865 -0.054707029 -0.054792878 -0.054885280 #> V3         0.1126396902  0.112669242  0.112713333  0.112738691  0.112767919 #> V4        -0.0501774236 -0.050220889 -0.050293763 -0.050332180 -0.050374860 #> V5         0.0275341240  0.027554061  0.027580476  0.027596520  0.027616533 #> V6        -0.0629842332 -0.063039152 -0.063130047 -0.063178613 -0.063232622 #> V7         0.0923780803  0.092428389  0.092513906  0.092558131  0.092608164 #> V8         0.0920764245  0.092098922  0.092128489  0.092146690  0.092169430 #> V9        -0.0625740832 -0.062600773 -0.062639569 -0.062662695 -0.062688817 #> V10       -0.2571513168 -0.257236969 -0.257393208 -0.257469763 -0.257554826 #> V11       -0.1191235997 -0.119180838 -0.119287443 -0.119339260 -0.119395655 #> V12       -0.0546999817 -0.054767248 -0.054894860 -0.054957263 -0.055023107 #> V13       -0.0064054860 -0.006489576 -0.006648094 -0.006724157 -0.006806869 #> V14       -0.0364201967 -0.036464817 -0.036541884 -0.036580972 -0.036625039 #> V15        0.1597598302  0.159838671  0.159984711  0.160055716  0.160133501 #> V16        0.0061269342  0.006179357  0.006271895  0.006318259  0.006369829 #> V17       -0.1693590255 -0.169410063 -0.169497112 -0.169541502 -0.169591806 #> V18       -0.0585319558 -0.058564008 -0.058617332 -0.058645496 -0.058677065 #> V19       -0.2316261548 -0.231694275 -0.231815631 -0.231876604 -0.231943956 #> V20        0.0008553733  0.000932770  0.001074418  0.001144922  0.001221073 #>                0.00065      0.00060      0.00056      0.00053      0.00049 #> intercept  0.105454336  0.105434479  0.105414919  0.105395944  0.105377724 #> V1        -0.115214158 -0.115277977 -0.115339596 -0.115398290 -0.115453753 #> V2        -0.054980132 -0.055074756 -0.055167448 -0.055257139 -0.055343179 #> V3         0.112797097  0.112824834  0.112850702  0.112874660  0.112896815 #> V4        -0.050418137 -0.050460418 -0.050500945 -0.050539365 -0.050575542 #> V5         0.027636431  0.027654774  0.027671222  0.027685850  0.027698860 #> V6        -0.063287075 -0.063339985 -0.063390534 -0.063438407 -0.063483520 #> V7         0.092659146  0.092708828  0.092756227  0.092800972  0.092842982 #> V8         0.092192158  0.092213216  0.092232147  0.092248976  0.092263905 #> V9        -0.062714493 -0.062738754 -0.062761413 -0.062782519 -0.062802183 #> V10       -0.257642560 -0.257729450 -0.257813551 -0.257893865 -0.257969943 #> V11       -0.119453768 -0.119511705 -0.119568270 -0.119622740 -0.119674707 #> V12       -0.055090446 -0.055157929 -0.055224528 -0.055289463 -0.055352173 #> V13       -0.006892394 -0.006977891 -0.007061492 -0.007142058 -0.007218943 #> V14       -0.036670184 -0.036714419 -0.036756769 -0.036796801 -0.036834376 #> V15        0.160213706  0.160293501  0.160371178  0.160445769  0.160516769 #> V16        0.006422721  0.006474841  0.006525074  0.006572860  0.006617962 #> V17       -0.169643383 -0.169693870 -0.169742091 -0.169787523 -0.169830000 #> V18       -0.058709049 -0.058740174 -0.058769885 -0.058797957 -0.058824325 #> V19       -0.232012864 -0.232080813 -0.232146492 -0.232209234 -0.232268731 #> V20        0.001298929  0.001376276  0.001451774  0.001524602  0.001594267 #>                0.00046      0.00043      0.00040      0.00037      0.00035 #> intercept  0.105360353  0.105343873  0.105328296  0.105313615  0.105299806 #> V1        -0.115505905 -0.115554798 -0.115600547 -0.115643303 -0.115683231 #> V2        -0.055425194 -0.055503006 -0.055576562 -0.055645903 -0.055711127 #> V3         0.112917320  0.112936329  0.112953984  0.112970407  0.112985702 #> V4        -0.050609464 -0.050641185 -0.050670801 -0.050698423 -0.050724169 #> V5         0.027710475  0.027720897  0.027730300  0.027738827  0.027746596 #> V6        -0.063525907 -0.063565657 -0.063602887 -0.063637726 -0.063670306 #> V7         0.092882311  0.092919075  0.092953413  0.092985471  0.093015392 #> V8         0.092277177  0.092289028  0.092299664  0.092309260  0.092317961 #> V9        -0.062820524 -0.062837647 -0.062853649 -0.062868610 -0.062882603 #> V10       -0.258041643 -0.258109003 -0.258172149 -0.258231261 -0.258286543 #> V11       -0.119723965 -0.119770446 -0.119814164 -0.119855189 -0.119893622 #> V12       -0.055412276 -0.055469541 -0.055523848 -0.055575161 -0.055623509 #> V13       -0.007291829 -0.007360608 -0.007425304 -0.007486019 -0.007542908 #> V14       -0.036869506 -0.036902281 -0.036932827 -0.036961283 -0.036987789 #> V15        0.160583957  0.160647288  0.160706821  0.160762676  0.160815007 #> V16        0.006660323  0.006699987  0.006737056  0.006771658  0.006803935 #> V17       -0.169869545 -0.169906280 -0.169940371 -0.169972001 -0.170001351 #> V18       -0.058849009 -0.058872070 -0.058893589 -0.058913656 -0.058932359 #> V19       -0.232324877 -0.232377687 -0.232427244 -0.232473670 -0.232517112 #> V20        0.001660503  0.001723194  0.001782330  0.001837968  0.001890214 #>                0.00032      0.00030      0.00028      0.00026      0.00024 #> intercept  0.105286839  0.105274678  0.105263286  0.105252622  0.105242647 #> V1        -0.115720498 -0.115755271 -0.115787709 -0.115817964 -0.115846181 #> V2        -0.055772373 -0.055829808 -0.055883609 -0.055933965 -0.055981063 #> V3         0.112999962  0.113013265  0.113025680  0.113037269  0.113048089 #> V4        -0.050748160 -0.050770512 -0.050791335 -0.050810734 -0.050828808 #> V5         0.027753703  0.027760228  0.027766235  0.027771781  0.027776911 #> V6        -0.063700761 -0.063729217 -0.063755799 -0.063780624 -0.063803803 #> V7         0.093043316  0.093069371  0.093093683  0.093116365  0.093137526 #> V8         0.092325883  0.092333126  0.092339769  0.092345878  0.092351511 #> V9        -0.062895693 -0.062907937 -0.062919389 -0.062930099 -0.062940112 #> V10       -0.258338206 -0.258386463 -0.258431521 -0.258473580 -0.258512832 #> V11       -0.119929583 -0.119963201 -0.119994610 -0.120023940 -0.120051319 #> V12       -0.055668962 -0.055711619 -0.055751598 -0.055789028 -0.055824042 #> V13       -0.007596146 -0.007645927 -0.007692445 -0.007735893 -0.007776460 #> V14       -0.037012480 -0.037035484 -0.037056919 -0.037076895 -0.037095514 #> V15        0.160863988  0.160909796  0.160952614  0.160992619  0.161029982 #> V16        0.006834030  0.006862084  0.006888232  0.006912603  0.006935317 #> V17       -0.170028595 -0.170053896 -0.170077404 -0.170099256 -0.170119578 #> V18       -0.058949789 -0.058966031 -0.058981165 -0.058995267 -0.059008408 #> V19       -0.232557724 -0.232595668 -0.232631100 -0.232664176 -0.232695044 #> V20        0.001939199  0.001985073  0.002027996  0.002068128  0.002105631 #>                0.00023      0.00021      0.00020      0.00018      0.00017 #> intercept  0.105233321  0.105224605  0.105216464  0.105208860  0.105201761 #> V1        -0.115872495 -0.115897034 -0.115919917 -0.115941254 -0.115961152 #> V2        -0.056025089 -0.056066227 -0.056104651 -0.056140531 -0.056174027 #> V3         0.113058192  0.113067624  0.113076430  0.113084650  0.113092323 #> V4        -0.050845649 -0.050861341 -0.050875965 -0.050889593 -0.050902296 #> V5         0.027781664  0.027786073  0.027790170  0.027793978  0.027797521 #> V6        -0.063825443 -0.063845642 -0.063864493 -0.063882086 -0.063898503 #> V7         0.093157267  0.093175683  0.093192862  0.093208887  0.093223834 #> V8         0.092356713  0.092361526  0.092365985  0.092370120  0.092373959 #> V9        -0.062949471 -0.062958218 -0.062966391 -0.062974026 -0.062981156 #> V10       -0.258549458 -0.258583630 -0.258615508 -0.258645244 -0.258672981 #> V11       -0.120076871 -0.120100713 -0.120122957 -0.120143707 -0.120163063 #> V12       -0.055856775 -0.055887360 -0.055915927 -0.055942600 -0.055967499 #> V13       -0.007814327 -0.007849666 -0.007882641 -0.007913407 -0.007942110 #> V14       -0.037112872 -0.037129054 -0.037144143 -0.037158212 -0.037171332 #> V15        0.161064868  0.161097435  0.161127831  0.161156197  0.161182667 #> V16        0.006956488  0.006976222  0.006994617  0.007011765  0.007027751 #> V17       -0.170138485 -0.170156081 -0.170172463 -0.170187719 -0.170201929 #> V18       -0.059020655 -0.059032068 -0.059042706 -0.059052621 -0.059061864 #> V19       -0.232723845 -0.232750714 -0.232775779 -0.232799157 -0.232820962 #> V20        0.002140662  0.002173373  0.002203909  0.002232409  0.002259004 predict(fit, X = X[1:5, ], lambda = 0.1) #> [1] 0.22603424 0.04784565 0.33822276 0.28862813 0.14906688  # Binomial X <- matrix(rnorm(100 * 20), 100, 20) y <- sample(c(0,1), 100, replace = TRUE) fit <- glmtlp(X, y, family = \"binomial\", penalty = \"l1\") coef(fit) #>           0.110373    0.102934   0.095997   0.089527   0.083493    0.077866 #> intercept        0 0.006107365 0.01184730 0.01723389 0.02238031 0.028581394 #> V1               0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V2               0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V3               0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V4               0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V5               0 0.000000000 0.00000000 0.00000000 0.00000000 0.007267387 #> V6               0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V7               0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V8               0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V9               0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V10              0 0.038434487 0.07438146 0.10802302 0.14138922 0.172848257 #> V11              0 0.000000000 0.00000000 0.00000000 0.02168670 0.047713265 #> V12              0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V13              0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V14              0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V15              0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V16              0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V17              0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V18              0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V19              0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #> V20              0 0.000000000 0.00000000 0.00000000 0.00000000 0.000000000 #>             0.072618   0.067724   0.063159     0.058903    0.054933    0.051231 #> intercept 0.03772963 0.04638474 0.05458541  0.061163389  0.06931718  0.07562829 #> V1        0.00000000 0.00000000 0.00000000  0.000000000  0.00000000  0.00000000 #> V2        0.00000000 0.00000000 0.00000000  0.000000000  0.00000000  0.00000000 #> V3        0.00000000 0.00000000 0.00000000  0.000000000  0.00000000  0.00000000 #> V4        0.00000000 0.00000000 0.00000000  0.000000000  0.00000000  0.00000000 #> V5        0.03159199 0.05442496 0.07591723  0.100812521  0.12535473  0.14876193 #> V6        0.00000000 0.00000000 0.00000000  0.000000000  0.00000000  0.00000000 #> V7        0.00000000 0.00000000 0.00000000  0.000000000  0.00000000  0.00000000 #> V8        0.00000000 0.00000000 0.00000000 -0.001759608 -0.01737293 -0.03351756 #> V9        0.00000000 0.00000000 0.00000000  0.000000000  0.00000000  0.00000000 #> V10       0.20163735 0.22886505 0.25461136  0.279957664  0.30472710  0.32890185 #> V11       0.07336928 0.09744672 0.12007480  0.140369932  0.15799796  0.17439420 #> V12       0.00000000 0.00000000 0.00000000  0.022723596  0.04105618  0.05584513 #> V13       0.00000000 0.00000000 0.00000000  0.000000000  0.00000000  0.01207948 #> V14       0.00000000 0.00000000 0.00000000  0.000000000  0.00000000  0.00000000 #> V15       0.00000000 0.00000000 0.00000000  0.000000000  0.00000000  0.00000000 #> V16       0.00000000 0.00000000 0.00000000  0.000000000  0.00000000  0.00000000 #> V17       0.00000000 0.00000000 0.00000000  0.000000000  0.00000000 -0.01073705 #> V18       0.00000000 0.00000000 0.00000000  0.000000000  0.00000000  0.00000000 #> V19       0.00000000 0.00000000 0.00000000  0.000000000  0.00000000  0.00000000 #> V20       0.00000000 0.00000000 0.00000000  0.000000000  0.00000000  0.00000000 #>              0.047778    0.044558     0.041555    0.038754    0.036142 #> intercept  0.08130568  0.08690462  0.094622236  0.10180211  0.10868465 #> V1         0.00000000  0.00000000  0.000000000  0.00000000  0.00000000 #> V2         0.00000000  0.00000000 -0.015402057 -0.03484898 -0.05317008 #> V3         0.00000000  0.00000000  0.000000000  0.00000000  0.00000000 #> V4         0.00000000  0.00000000  0.000000000  0.00000000  0.00000000 #> V5         0.17101632  0.19235368  0.211595593  0.22800614  0.24382195 #> V6         0.00000000  0.00000000  0.000000000  0.00000000  0.00000000 #> V7         0.00000000  0.00000000  0.000000000  0.00000000  0.00000000 #> V8        -0.04926306 -0.06418618 -0.081107169 -0.09768308 -0.11338811 #> V9         0.00000000  0.00000000  0.000000000  0.00000000  0.00000000 #> V10        0.35209286  0.37439715  0.398435402  0.42364746  0.44780107 #> V11        0.18984928  0.20457783  0.219230936  0.23331017  0.24672909 #> V12        0.06893586  0.08138962  0.092092272  0.10218843  0.11179590 #> V13        0.02617009  0.03941174  0.053192531  0.06659885  0.07924699 #> V14        0.00000000  0.00000000  0.006431744  0.02094686  0.03465842 #> V15        0.00000000  0.00000000  0.000000000  0.00000000  0.00000000 #> V16        0.00000000  0.00000000  0.000000000  0.00000000  0.00000000 #> V17       -0.02401870 -0.03669277 -0.050435636 -0.06359398 -0.07613132 #> V18        0.00000000  0.00000000  0.000000000  0.00000000  0.00000000 #> V19        0.00000000  0.00000000  0.000000000  0.00000000  0.00000000 #> V20        0.00000000  0.00000000  0.000000000  0.00000000  0.00000000 #>              0.033706     0.031435     0.029316    0.027340     0.025498 #> intercept  0.11527596  0.120891070  0.126917201  0.13365593  0.139258578 #> V1         0.00000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V2        -0.07048512 -0.086944811 -0.102044431 -0.11603700 -0.129101187 #> V3         0.00000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V4         0.00000000  0.000000000  0.005168056  0.01394221  0.022525620 #> V5         0.25899512  0.272686485  0.286164572  0.30006926  0.313350873 #> V6         0.00000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V7         0.00000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V8        -0.12834082 -0.142769801 -0.156049212 -0.16823274 -0.179679345 #> V9         0.00000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V10        0.47095765  0.492133964  0.512300976  0.53220046  0.551741249 #> V11        0.25951188  0.271632685  0.282458830  0.29231156  0.301327464 #> V12        0.12089971  0.129768664  0.138558655  0.14723822  0.155027729 #> V13        0.09124748  0.103475628  0.113985325  0.12245056  0.129997364 #> V14        0.04765620  0.059138137  0.069917786  0.08025039  0.090725102 #> V15        0.00000000  0.000000000  0.000000000  0.00000000 -0.005837813 #> V16        0.00000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V17       -0.08807713 -0.099328270 -0.110501480 -0.12184789 -0.133141607 #> V18        0.00000000  0.000000000  0.000000000  0.00000000  0.000000000 #> V19        0.00000000  0.005583924  0.011973814  0.01813536  0.022740101 #> V20        0.00000000  0.000000000  0.000000000  0.00000000  0.000000000 #>              0.023779      0.022176     0.020682    0.019288     0.017988 #> intercept  0.14414509  0.1491364562  0.154449195  0.15961145  0.164806265 #> V1         0.00000000  0.0000000000  0.000000000  0.00000000  0.000000000 #> V2        -0.14126919 -0.1529140465 -0.163940698 -0.17442725 -0.184254298 #> V3         0.00000000  0.0009772158  0.006989036  0.01260901  0.017802693 #> V4         0.03071709  0.0386095826  0.046248954  0.05356774  0.060788903 #> V5         0.32582641  0.3378856926  0.348949387  0.35966224  0.370200316 #> V6         0.00000000  0.0000000000  0.000000000  0.00000000  0.000000000 #> V7         0.00000000  0.0000000000  0.000000000  0.00000000  0.000000000 #> V8        -0.19032725 -0.2006317081 -0.210860787 -0.22058794 -0.229857948 #> V9         0.00000000  0.0000000000  0.000000000  0.00000000  0.001533309 #> V10        0.57051704  0.5886638867  0.606061699  0.62284311  0.639093504 #> V11        0.30971392  0.3177665937  0.325528694  0.33291177  0.339838004 #> V12        0.16231279  0.1692371927  0.175585060  0.18165689  0.187262196 #> V13        0.13697540  0.1435301864  0.149329233  0.15481585  0.159853530 #> V14        0.10101675  0.1107495435  0.119965462  0.12874808  0.137144043 #> V15       -0.01320331 -0.0199534622 -0.026042235 -0.03176993 -0.037215435 #> V16        0.00000000  0.0000000000  0.000000000  0.00000000  0.000000000 #> V17       -0.14396781 -0.1543266006 -0.164080723 -0.17342279 -0.182284584 #> V18        0.00000000  0.0000000000  0.000000000  0.00000000  0.000000000 #> V19        0.02667254  0.0304497883  0.033880394  0.03713976  0.040143000 #> V20        0.00000000  0.0000000000  0.000000000  0.00000000  0.000000000 #>               0.016776     0.015645      0.014591     0.013607    0.012690 #> intercept  0.170490327  0.176519878  0.1821180990  0.187431799  0.19254069 #> V1         0.000000000  0.000000000  0.0000000000  0.000000000  0.00000000 #> V2        -0.192760377 -0.200184924 -0.2070426221 -0.213518152 -0.21969998 #> V3         0.022542470  0.027120178  0.0307309212  0.034616830  0.03827699 #> V4         0.068047223  0.074024630  0.0794313284  0.083948002  0.08828396 #> V5         0.380657433  0.390404963  0.4000059083  0.407989729  0.41569111 #> V6         0.002763169  0.011594599  0.0198923221  0.028599815  0.03682098 #> V7         0.000000000 -0.001482458 -0.0071506747 -0.011650565 -0.01595619 #> V8        -0.238840091 -0.248160216 -0.2573920449 -0.265462090 -0.27317568 #> V9         0.006615977  0.011450694  0.0161861551  0.021488842  0.02651486 #> V10        0.654828046  0.670412272  0.6862858503  0.701770963  0.71666685 #> V11        0.346021249  0.352271997  0.3588336183  0.364129609  0.36918730 #> V12        0.192207453  0.197415842  0.2025640628  0.207476966  0.21218141 #> V13        0.164647465  0.171011664  0.1778173229  0.185537432  0.19284882 #> V14        0.145304634  0.153198929  0.1607679706  0.168374550  0.17561869 #> V15       -0.042428329 -0.046725027 -0.0508312731 -0.053814765 -0.05662509 #> V16        0.000000000  0.000000000  0.0002858612  0.005309256  0.01005384 #> V17       -0.190386896 -0.197875582 -0.2050787662 -0.211927830 -0.21846961 #> V18        0.000000000  0.000000000  0.0000000000  0.000000000  0.00000000 #> V19        0.042964982  0.046288142  0.0487708684  0.051427547  0.05394446 #> V20        0.000000000  0.002908440  0.0100514845  0.016700541  0.02303772 #>              0.011835    0.011037    0.010293    0.009600    0.008953 #> intercept  0.19739280  0.20199668  0.20629223  0.21042912  0.21438872 #> V1         0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 #> V2        -0.22556873 -0.23113715 -0.23637954 -0.24138554 -0.24615355 #> V3         0.04173408  0.04499879  0.04806734  0.05097665  0.05373070 #> V4         0.09240133  0.09630739  0.09997596  0.10348469  0.10682980 #> V5         0.42301882  0.42998386  0.43652465  0.44280426  0.44880862 #> V6         0.04458956  0.05192929  0.05882579  0.06537308  0.07157869 #> V7        -0.02002813 -0.02387683 -0.02748984 -0.03092448 -0.03418197 #> V8        -0.28048510 -0.28740720 -0.29390814 -0.30010641 -0.30599876 #> V9         0.03129295  0.03583374  0.04012022  0.04421613  0.04812240 #> V10        0.73091202  0.74451932  0.75738303  0.76976319  0.78163702 #> V11        0.37397165  0.37849505  0.38275353  0.38679169  0.39061592 #> V12        0.21667863  0.22097348  0.22505858  0.22896452  0.23269623 #> V13        0.19981113  0.20643794  0.21272998  0.21872167  0.22442459 #> V14        0.18251163  0.18906565  0.19526076  0.20117463  0.20681023 #> V15       -0.05925926 -0.06172855 -0.06405744 -0.06622842 -0.06825742 #> V16        0.01460269  0.01895991  0.02310639  0.02709177  0.03091465 #> V17       -0.22468554 -0.23058762 -0.23614452 -0.24145695 -0.24652257 #> V18        0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 #> V19        0.05634863  0.05864388  0.06084189  0.06292798  0.06491146 #> V20        0.02906480  0.03479422  0.04022118  0.04539017  0.05030957 #>              0.008349    0.007787    0.007262    0.006772    0.006316 #> intercept  0.21801691  0.22152224  0.22483738  0.22796816  0.23092288 #> V1         0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 #> V2        -0.25060152 -0.25484839 -0.25886466 -0.26265963 -0.26624328 #> V3         0.05630410  0.05874467  0.06104482  0.06321152  0.06525165 #> V4         0.10993839  0.11291070  0.11572123  0.11837554  0.12088063 #> V5         0.45437479  0.45972063  0.46478312  0.46957101  0.47409594 #> V6         0.07736905  0.08286915  0.08805511  0.09294226  0.09754583 #> V7        -0.03721807 -0.04010385 -0.04282539 -0.04538991 -0.04780530 #> V8        -0.31147916 -0.31670687 -0.32164173 -0.32629626 -0.33068395 #> V9         0.05178005  0.05527315  0.05858221  0.06171424  0.06467672 #> V10        0.79272426  0.80338285  0.81350639  0.82310952  0.83221068 #> V11        0.39419691  0.39758950  0.40078787  0.40380134  0.40663923 #> V12        0.23622356  0.23958484  0.24277620  0.24580330  0.24867204 #> V13        0.22981243  0.23493096  0.23977901  0.24436840  0.24870978 #> V14        0.21209709  0.21713633  0.22190746  0.22642086  0.23068741 #> V15       -0.07018602 -0.07197624 -0.07365465 -0.07522835 -0.07670367 #> V16        0.03451813  0.03797328  0.04126061  0.04438502  0.04735159 #> V17       -0.25124635 -0.25576112 -0.26003387 -0.26407352 -0.26789034 #> V18        0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 #> V19        0.06680984  0.06860301  0.07030388  0.07191660  0.07344446 #> V20        0.05494437  0.05934974  0.06352066  0.06746687  0.07119788 #>              0.005890      0.005493     0.005123     0.004778     0.004456 #> intercept  0.23370972  0.2363867755  0.238564981  0.240717669  0.243028255 #> V1         0.00000000 -0.0008359585 -0.003428182 -0.005807325 -0.008099526 #> V2        -0.26962535 -0.2729427424 -0.276249242 -0.279381571 -0.282402901 #> V3         0.06717182  0.0689146251  0.070265751  0.071572092  0.072673966 #> V4         0.12324347  0.1256635092  0.128352813  0.130881263  0.133319922 #> V5         0.47836950  0.4823752863  0.485545042  0.488664683  0.491675601 #> V6         0.10188041  0.1060950803  0.110209111  0.114106740  0.117753963 #> V7        -0.05007917 -0.0522770732 -0.054482906 -0.056564974 -0.058574884 #> V8        -0.33481789 -0.3387535636 -0.342169929 -0.345464633 -0.348782102 #> V9         0.06747698  0.0702329031  0.072968985  0.075566696  0.078087392 #> V10        0.84082861  0.8491887663  0.857049383  0.864595175  0.871781046 #> V11        0.40931050  0.4118748606  0.414279425  0.416561719  0.418637506 #> V12        0.25138836  0.2539785314  0.256460732  0.258812802  0.260841686 #> V13        0.25281359  0.2566394672  0.259973665  0.263145076  0.266342294 #> V14        0.23471785  0.2386613471  0.242799631  0.246684044  0.250401691 #> V15       -0.07808656 -0.0793579755 -0.080716872 -0.081955023 -0.082974742 #> V16        0.05016554  0.0528433218  0.055297788  0.057642505  0.060062593 #> V17       -0.27149441 -0.2749544044 -0.278107721 -0.281133987 -0.284104090 #> V18        0.00000000  0.0000000000  0.000000000  0.000000000 -0.001265699 #> V19        0.07489070  0.0761974440  0.077228163  0.078203843  0.079129279 #> V20        0.07472303  0.0779340328  0.080455796  0.082894419  0.085214724 #>               0.004155     0.003875     0.003614     0.003371     0.003143 #> intercept  0.245134327  0.247211903  0.249170724  0.251012211  0.252742353 #> V1        -0.010156950 -0.012129081 -0.013978978 -0.015711507 -0.017333623 #> V2        -0.285194347 -0.287872661 -0.290394910 -0.292767305 -0.294997595 #> V3         0.073584283  0.074464284  0.075288895  0.076061559  0.076785408 #> V4         0.135387527  0.137409940  0.139314478  0.141101681  0.142777814 #> V5         0.494206232  0.496707172  0.499068843  0.501290732  0.503379602 #> V6         0.121073669  0.124250757  0.127234121  0.130034432  0.132662068 #> V7        -0.060321362 -0.062018009 -0.063615718 -0.065115932 -0.066523829 #> V8        -0.352032531 -0.355116687 -0.358017194 -0.360743331 -0.363304461 #> V9         0.080461256  0.082715461  0.084834923  0.086825869  0.088695291 #> V10        0.878078890  0.884229494  0.890036512  0.895507611  0.900658653 #> V11        0.420461708  0.422208595  0.423850104  0.425391384  0.426837968 #> V12        0.262473642  0.264069210  0.265575216  0.266994301  0.268330539 #> V13        0.269766016  0.272919180  0.275879608  0.278664073  0.281282227 #> V14        0.253735032  0.256948873  0.259975982  0.262822782  0.265498573 #> V15       -0.083752437 -0.084489276 -0.085179114 -0.085822966 -0.086423721 #> V16        0.062534235  0.064854873  0.067038131  0.069092731  0.071025372 #> V17       -0.286797636 -0.289395090 -0.291843746 -0.294147213 -0.296312807 #> V18       -0.003195019 -0.004967713 -0.006631435 -0.008193356 -0.009659247 #> V19        0.080199596  0.081153289  0.082048046  0.082891295  0.083685800 #> V20        0.087478003  0.089595953  0.091589407  0.093466504  0.095233157 #>              0.002932    0.002734    0.002550    0.002378    0.002218 #> intercept  0.25436716  0.25585739  0.25728855  0.25863292  0.25989366 #> V1        -0.01885205 -0.02026162 -0.02159153 -0.02283620 -0.02400047 #> V2        -0.29709332 -0.29904095 -0.30088906 -0.30262448 -0.30425260 #> V3         0.07746337  0.07809627  0.07869068  0.07924709  0.07976780 #> V4         0.14434925  0.14580791  0.14718767  0.14848108  0.14969241 #> V5         0.50534245  0.50716343  0.50889381  0.51051926  0.51204426 #> V6         0.13512685  0.13741760  0.13958463  0.14161622  0.14351964 #> V7        -0.06784461 -0.06907337 -0.07023442 -0.07132326 -0.07234350 #> V8        -0.36570954 -0.36793874 -0.37005703 -0.37204545 -0.37391000 #> V9         0.09044993  0.09208253  0.09362669  0.09507515  0.09643284 #> V10        0.90550555  0.91001065  0.91429534  0.91832334  0.92210614 #> V11        0.42819517  0.42946593  0.43065938  0.43177800  0.43282614 #> V12        0.26958801  0.27077402  0.27188577  0.27293001  0.27391065 #> V13        0.28374285  0.28604354  0.28821438  0.29025208  0.29216392 #> V14        0.26801248  0.27035501  0.27257133  0.27465174  0.27660323 #> V15       -0.08698426 -0.08752191 -0.08800977 -0.08846434 -0.08888845 #> V16        0.07284237  0.07453396  0.07613799  0.07764446  0.07905825 #> V17       -0.29834782 -0.30024177 -0.30203624 -0.30372140 -0.30530243 #> V18       -0.01103452 -0.01231218 -0.01352139 -0.01465538 -0.01571793 #> V19        0.08443393  0.08514306  0.08580534  0.08642749  0.08701203 #> V20        0.09689495  0.09845711  0.09992524  0.10130405  0.10259851 #>              0.002068    0.001929    0.001799    0.001678    0.001565 #> intercept  0.26107547  0.26214897  0.26318585  0.26415940  0.26507114 #> V1        -0.02508933 -0.02609598 -0.02704797 -0.02793850 -0.02877090 #> V2        -0.30577946 -0.30718996 -0.30853127 -0.30978924 -0.31096760 #> V3         0.08025500  0.08070883  0.08113515  0.08153390  0.08190674 #> V4         0.15082652  0.15187435  0.15286734  0.15379755  0.15466782 #> V5         0.51347440  0.51479291  0.51604855  0.51722684  0.51833063 #> V6         0.14530248  0.14695086  0.14851396  0.14997798  0.15134803 #> V7        -0.07329916 -0.07418467 -0.07502218 -0.07580690 -0.07654132 #> V8        -0.37565774 -0.37726742 -0.37880115 -0.38023948 -0.38158632 #> V9         0.09770504  0.09888297  0.09999899  0.10104470  0.10202359 #> V10        0.92565700  0.92893427  0.93205891  0.93499182  0.93774106 #> V11        0.43380797  0.43472459  0.43558549  0.43639137  0.43714552 #> V12        0.27483117  0.27569694  0.27650726  0.27726664  0.27797837 #> V13        0.29395703  0.29562591  0.29720194  0.29867895  0.30006245 #> V14        0.27843311  0.28012932  0.28173675  0.28324356  0.28465483 #> V15       -0.08928416 -0.08966770 -0.09001223 -0.09033282 -0.09063187 #> V16        0.08038454  0.08161183  0.08277792  0.08387145  0.08489595 #> V17       -0.30678520 -0.30815752 -0.30945998 -0.31068157 -0.31182591 #> V18       -0.01671325 -0.01763344 -0.01850583 -0.01932326 -0.02008825 #> V19        0.08756100  0.08808031  0.08856412  0.08901744  0.08944244 #> V20        0.10381329  0.10495142  0.10602014  0.10702184  0.10796052 #>              0.001459    0.001361    0.001269    0.001183    0.001104 #> intercept  0.26592463  0.26669103  0.26743705  0.26813796  0.26879383 #> V1        -0.02954882 -0.03026430 -0.03094336 -0.03157852 -0.03217191 #> V2        -0.31207101 -0.31308332 -0.31404953 -0.31495529 -0.31580279 #> V3         0.08225531  0.08257928  0.08288377  0.08316844  0.08343443 #> V4         0.15548177  0.15622976  0.15694065  0.15760655  0.15822910 #> V5         0.51936422  0.52031043  0.52121471  0.52206312  0.52285710 #> V6         0.15262982  0.15380752  0.15492867  0.15597839  0.15695987 #> V7        -0.07722845 -0.07786236 -0.07846292 -0.07902547 -0.07955152 #> V8        -0.38284704 -0.38399971 -0.38510256 -0.38613667 -0.38710407 #> V9         0.10293967  0.10378304  0.10458452  0.10533516  0.10603717 #> V10        0.94031712  0.94267540  0.94493387  0.94705238  0.94903565 #> V11        0.43785111  0.43850765  0.43912502  0.43970240  0.44024221 #> V12        0.27864525  0.27927073  0.27985603  0.28040356  0.28091597 #> V13        0.30135801  0.30255759  0.30369305  0.30475613  0.30575070 #> V14        0.28597621  0.28719360  0.28835097  0.28943513  0.29044942 #> V15       -0.09091087 -0.09118483 -0.09142802 -0.09165377 -0.09186428 #> V16        0.08585547  0.08673719  0.08757812  0.08836614  0.08910352 #> V17       -0.31289750 -0.31388294 -0.31482122 -0.31570076 -0.31652379 #> V18       -0.02080399 -0.02146209 -0.02208774 -0.02267384 -0.02322189 #> V19        0.08984080  0.09021703  0.09056708  0.09089435  0.09120067 #> V20        0.10883989  0.10966091  0.11043222  0.11115411  0.11182966 #>              0.001029    0.000960    0.000895    0.000835    0.000779 #> intercept  0.26940718  0.26995017  0.27048399  0.27098661  0.27145680 #> V1        -0.03272616 -0.03323247 -0.03371550 -0.03416753 -0.03458970 #> V2        -0.31659552 -0.31731666 -0.31800873 -0.31865778 -0.31926469 #> V3         0.08368294  0.08391343  0.08413019  0.08433282  0.08452206 #> V4         0.15881092  0.15934209  0.15984897  0.16032407  0.16076810 #> V5         0.52359976  0.52427387  0.52492138  0.52552935  0.52609803 #> V6         0.15787735  0.15871357  0.15951427  0.16026423  0.16096507 #> V7        -0.08004327 -0.08049468 -0.08092337 -0.08132508 -0.08170055 #> V8        -0.38800871 -0.38882850 -0.38961733 -0.39035764 -0.39104984 #> V9         0.10669353  0.10729351  0.10786635  0.10840298  0.10890457 #> V10        0.95089156  0.95257350  0.95419513  0.95571685  0.95714031 #> V11        0.44074682  0.44121437  0.44165519  0.44206722  0.44245219 #> V12        0.28139545  0.28184375  0.28226380  0.28265618  0.28302298 #> V13        0.30668098  0.30753684  0.30835028  0.30911161  0.30982328 #> V14        0.29139808  0.29226542  0.29309422  0.29387062  0.29459646 #> V15       -0.09206065 -0.09225662 -0.09242828 -0.09258697 -0.09273484 #> V16        0.08979331  0.09042172  0.09102454  0.09158953  0.09211779 #> V17       -0.31729364 -0.31799602 -0.31866811 -0.31929831 -0.31988764 #> V18       -0.02373420 -0.02420219 -0.02464882 -0.02506747 -0.02545876 #> V19        0.09148738  0.09175770  0.09200924  0.09224395  0.09246335 #> V20        0.11246171  0.11304939  0.11360260  0.11411985  0.11460341 #>              0.000726    0.000677    0.000632    0.000589    0.000549 #> intercept  0.27189623  0.27227826  0.27265844  0.27301792  0.27335440 #> V1        -0.03498387 -0.03534061 -0.03568343 -0.03600467 -0.03630469 #> V2        -0.31983192 -0.32034230 -0.32083581 -0.32129939 -0.32173282 #> V3         0.08469878  0.08486234  0.08501624  0.08516015  0.08529452 #> V4         0.16118287  0.16155829  0.16191853  0.16225674  0.16257286 #> V5         0.52662956  0.52710679  0.52756832  0.52800255  0.52840876 #> V6         0.16161979  0.16221013  0.16278001  0.16331455  0.16381400 #> V7        -0.08205133 -0.08237138 -0.08267628 -0.08296233 -0.08322970 #> V8        -0.39169669 -0.39227626 -0.39283809 -0.39336652 -0.39386065 #> V9         0.10937321  0.10979763  0.11020552  0.11058811  0.11094565 #> V10        0.95847114  0.95966132  0.96081975  0.96190872  0.96292716 #> V11        0.44281182  0.44314308  0.44345685  0.44375010  0.44402396 #> V12        0.28336587  0.28368508  0.28398518  0.28426521  0.28452673 #> V13        0.31048840  0.31109505  0.31167534  0.31221874  0.31272644 #> V14        0.29527481  0.29588874  0.29647975  0.29703400  0.29755200 #> V15       -0.09287276 -0.09301317 -0.09313445 -0.09324581 -0.09334943 #> V16        0.09261154  0.09305630  0.09348645  0.09389016  0.09426751 #> V17       -0.32043847 -0.32093578 -0.32141510 -0.32186515 -0.32228595 #> V18       -0.02582431 -0.02615548 -0.02647315 -0.02677140 -0.02705017 #> V19        0.09266848  0.09286138  0.09304129  0.09320885  0.09336529 #> V20        0.11505543  0.11547332  0.11586835  0.11623752  0.11658238 #>              0.000512    0.000478    0.000446    0.000416    0.000388 #> intercept  0.27364019  0.27392871  0.27420354  0.27446135  0.27470231 #> V1        -0.03657270 -0.03683283 -0.03707727 -0.03730569 -0.03751890 #> V2        -0.32211709 -0.32249246 -0.32284629 -0.32317735 -0.32348658 #> V3         0.08541866  0.08553544  0.08564477  0.08574686  0.08584215 #> V4         0.16285570  0.16312917  0.16338668  0.16362758  0.16385257 #> V5         0.52876824  0.52911911  0.52945047  0.52976079  0.53005076 #> V6         0.16425763  0.16469073  0.16509826  0.16547925  0.16583499 #> V7        -0.08347178 -0.08370333 -0.08392108 -0.08412475 -0.08431495 #> V8        -0.39429688 -0.39472376 -0.39512692 -0.39550431 -0.39585686 #> V9         0.11126535  0.11157541  0.11186705  0.11213973  0.11239437 #> V10        0.96382173  0.96470377  0.96553617  0.96631523  0.96704310 #> V11        0.44427404  0.44451270  0.44473591  0.44494432  0.44513888 #> V12        0.28476858  0.28499744  0.28521087  0.28541004  0.28559600 #> V13        0.31318388  0.31362567  0.31404015  0.31442742  0.31478904 #> V14        0.29801427  0.29846399  0.29888687  0.29928224  0.29965148 #> V15       -0.09345742 -0.09354958 -0.09363328 -0.09371089 -0.09378319 #> V16        0.09460224  0.09492961  0.09523786  0.09552612  0.09579534 #> V17       -0.32266045 -0.32302514 -0.32336857 -0.32368989 -0.32399005 #> V18       -0.02729995 -0.02754120 -0.02776838 -0.02798089 -0.02817936 #> V19        0.09351173  0.09364908  0.09377678  0.09389586  0.09400702 #> V20        0.11689853  0.11719961  0.11748104  0.11774383  0.11798921 #>              0.000361    0.000337    0.000314    0.000293    0.000273 #> intercept  0.27490037  0.27510355  0.27529936  0.27548392  0.27565669 #> V1        -0.03770564 -0.03788936 -0.03806289 -0.03822529 -0.03837694 #> V2        -0.32375483 -0.32402039 -0.32427223 -0.32450834 -0.32472900 #> V3         0.08592997  0.08601251  0.08608994  0.08616228  0.08622980 #> V4         0.16405032  0.16424354  0.16442636  0.16459773  0.16475789 #> V5         0.53030184  0.53054994  0.53078566  0.53100699  0.53121398 #> V6         0.16614391  0.16645016  0.16674001  0.16701144  0.16726496 #> V7        -0.08448520 -0.08464891 -0.08480341 -0.08494816 -0.08508342 #> V8        -0.39616151 -0.39646308 -0.39674982 -0.39701896 -0.39727056 #> V9         0.11261776  0.11283709  0.11304443  0.11323860  0.11341998 #> V10        0.96766571  0.96829009  0.96888352  0.96944016  0.96996047 #> V11        0.44531405  0.44548320  0.44564173  0.44578976  0.44592793 #> V12        0.28576600  0.28592871  0.28608051  0.28622205  0.28635411 #> V13        0.31510872  0.31542186  0.31571681  0.31599262  0.31625014 #> V14        0.29997410  0.30029253  0.30059351  0.30087530  0.30113852 #> V15       -0.09386066 -0.09392645 -0.09398519 -0.09403925 -0.09408948 #> V16        0.09602882  0.09626055  0.09648008  0.09668572  0.09687783 #> V17       -0.32425142 -0.32450960 -0.32475402 -0.32498311 -0.32519722 #> V18       -0.02835427 -0.02852476 -0.02868610 -0.02883732 -0.02897863 #> V19        0.09411016  0.09420798  0.09429882  0.09438338  0.09446223 #> V20        0.11821101  0.11842485  0.11862503  0.11881192  0.11898635 #>              0.000255    0.000238    0.000222    0.000207    0.000193 #> intercept  0.27579236  0.27593383  0.27607261  0.27620475  0.27632902 #> V1        -0.03850571 -0.03863478 -0.03875782 -0.03887342 -0.03898153 #> V2        -0.32491415 -0.32510072 -0.32527951 -0.32544796 -0.32560571 #> V3         0.08629170  0.08634992  0.08640465  0.08645587  0.08650371 #> V4         0.16489475  0.16503059  0.16516012  0.16528205  0.16539624 #> V5         0.53138734  0.53156161  0.53172879  0.53188662  0.53203461 #> V6         0.16747756  0.16769272  0.16789852  0.16809209  0.16827319 #> V7        -0.08520228 -0.08531757 -0.08542693 -0.08552975 -0.08562599 #> V8        -0.39748123 -0.39769276 -0.39789605 -0.39808798 -0.39826788 #> V9         0.11357445  0.11372872  0.11387584  0.11401417  0.11414358 #> V10        0.97038816  0.97082651  0.97124842  0.97164635  0.97201912 #> V11        0.44604941  0.44616888  0.44628144  0.44638667  0.44648490 #> V12        0.28647235  0.28658770  0.28669567  0.28679630  0.28689011 #> V13        0.31647089  0.31669169  0.31690132  0.31709789  0.31728158 #> V14        0.30136105  0.30158512  0.30179895  0.30199993  0.30218792 #> V15       -0.09414484 -0.09419244 -0.09423396 -0.09427159 -0.09430630 #> V16        0.09703884  0.09720170  0.09735768  0.09750448  0.09764184 #> V17       -0.32537745 -0.32555908 -0.32573266 -0.32589605 -0.32604903 #> V18       -0.02909996 -0.02921985 -0.02933414 -0.02944174 -0.02954249 #> V19        0.09453410  0.09460365  0.09466832  0.09472839  0.09478432 #> V20        0.11914019  0.11929156  0.11943389  0.11956685  0.11969094 #>              0.000180    0.000168    0.000156    0.000146    0.000136 #> intercept  0.27642011  0.27654281  0.27662794  0.27671473  0.27680052 #> V1        -0.03906862 -0.03917254 -0.03925076 -0.03932972 -0.03940569 #> V2        -0.32573090 -0.32588320 -0.32599639 -0.32611082 -0.32622139 #> V3         0.08654686  0.08658938  0.08662723  0.08666313  0.08669679 #> V4         0.16548911  0.16559785  0.16568114  0.16576461  0.16584465 #> V5         0.53215180  0.53229446  0.53240051  0.53250766  0.53261113 #> V6         0.16841639  0.16859235  0.16872162  0.16885327  0.16898050 #> V7        -0.08570774 -0.08579673 -0.08586942 -0.08594043 -0.08600793 #> V8        -0.39841096 -0.39858334 -0.39871305 -0.39884295 -0.39896857 #> V9         0.11424838  0.11437225  0.11446635  0.11456093  0.11465185 #> V10        0.97230642  0.97267247  0.97293427  0.97320266  0.97346388 #> V11        0.44656756  0.44666050  0.44673358  0.44680676  0.44687633 #> V12        0.28697077  0.28705897  0.28712971  0.28720041  0.28726721 #> V13        0.31743075  0.31760768  0.31774096  0.31787621  0.31800590 #> V14        0.30233825  0.30251999  0.30265526  0.30279248  0.30292474 #> V15       -0.09434532 -0.09437318 -0.09440473 -0.09443412 -0.09445970 #> V16        0.09775066  0.09788371  0.09798210  0.09808181  0.09817827 #> V17       -0.32617070 -0.32631878 -0.32642844 -0.32653982 -0.32664727 #> V18       -0.02962513 -0.02972043 -0.02979449 -0.02986820 -0.02993880 #> V19        0.09483342  0.09488568  0.09492838  0.09497101  0.09501104 #> V20        0.11979545  0.11991325  0.12000544  0.12009825  0.12018635 #>              0.000127    0.000118    0.000110 #> intercept  0.27682924  0.27695516  0.27698276 #> V1        -0.03943896 -0.03954484 -0.03957393 #> V2        -0.32626732 -0.32642337 -0.32646401 #> V3         0.08671827  0.08675769  0.08677622 #> V4         0.16588051  0.16599070  0.16602200 #> V5         0.53265346  0.53279968  0.53283716 #> V6         0.16903195  0.16921382  0.16925903 #> V7        -0.08604409 -0.08613041 -0.08616193 #> V8        -0.39902350 -0.39919672 -0.39924595 #> V9         0.11469260  0.11481811  0.11485380 #> V10        0.97356147  0.97394202  0.97402942 #> V11        0.44691074  0.44700440  0.44703368 #> V12        0.28730186  0.28739072  0.28741977 #> V13        0.31806228  0.31824469  0.31829317 #> V14        0.30298074  0.30316734  0.30321632 #> V15       -0.09448339 -0.09450782 -0.09452620 #> V16        0.09821859  0.09835479  0.09839049 #> V17       -0.32669190 -0.32684405 -0.32688319 #> V18       -0.02997182 -0.03006739 -0.03009629 #> V19        0.09503272  0.09508537  0.09510329 #> V20        0.12022890  0.12034907  0.12038492 predict(fit, X = X[1:5, ], type = \"response\") #>      0.110373  0.102934  0.095997  0.089527  0.083493  0.077866  0.072618 #> [1,]      0.5 0.5024026 0.5046566 0.5067695 0.5034555 0.4966112 0.4842556 #> [2,]      0.5 0.4909916 0.4825783 0.4747172 0.4681113 0.4616825 0.4546858 #> [3,]      0.5 0.4974677 0.4951064 0.4929005 0.4878876 0.4809611 0.4708279 #> [4,]      0.5 0.4856075 0.4721746 0.4596429 0.4538326 0.4486508 0.4410905 #> [5,]      0.5 0.4967985 0.4938113 0.4910198 0.4887111 0.4885050 0.4928825 #>       0.067724  0.063159  0.058903  0.054933  0.051231  0.047778  0.044558 #> [1,] 0.4726978 0.4618698 0.4520518 0.4423076 0.4252340 0.4074601 0.3908564 #> [2,] 0.4480954 0.4418882 0.4363537 0.4278076 0.4188196 0.4102272 0.4021228 #> [3,] 0.4613339 0.4524283 0.4524853 0.4540570 0.4520935 0.4493612 0.4467695 #> [4,] 0.4339505 0.4272094 0.4145648 0.3994752 0.3839152 0.3691849 0.3553751 #> [5,] 0.4969844 0.5008440 0.4991183 0.5004234 0.4979292 0.4946918 0.4916773 #>       0.041555  0.038754  0.036142  0.033706  0.031435  0.029316  0.027340 #> [1,] 0.3723928 0.3550561 0.3389407 0.3239492 0.3093728 0.2961468 0.2843461 #> [2,] 0.3919312 0.3849927 0.3784304 0.3721999 0.3657679 0.3595246 0.3536911 #> [3,] 0.4385769 0.4283209 0.4186473 0.4094996 0.4004134 0.3910255 0.3816271 #> [4,] 0.3437390 0.3311109 0.3192586 0.3081205 0.2970599 0.2864859 0.2764816 #> [5,] 0.4865461 0.4825024 0.4787164 0.4751624 0.4735350 0.4712853 0.4685560 #>       0.025498  0.023779  0.022176  0.020682  0.019288  0.017988  0.016776 #> [1,] 0.2734780 0.2635126 0.2540508 0.2445401 0.2357262 0.2279721 0.2213990 #> [2,] 0.3480114 0.3427481 0.3379789 0.3349827 0.3320941 0.3297624 0.3294471 #> [3,] 0.3738222 0.3668321 0.3599868 0.3527376 0.3458754 0.3390365 0.3315391 #> [4,] 0.2663749 0.2566973 0.2474458 0.2377878 0.2287994 0.2199125 0.2105286 #> [5,] 0.4661151 0.4638408 0.4618918 0.4610900 0.4603110 0.4600733 0.4620533 #>       0.015645  0.014591  0.013607  0.012690  0.011835  0.011037  0.010293 #> [1,] 0.2141899 0.2076874 0.2009163 0.1946383 0.1888138 0.1834087 0.1784112 #> [2,] 0.3305020 0.3307124 0.3328192 0.3347512 0.3365742 0.3382950 0.3399181 #> [3,] 0.3242299 0.3178072 0.3109848 0.3045661 0.2985353 0.2928710 0.2875924 #> [4,] 0.2017148 0.1929565 0.1838414 0.1754537 0.1677323 0.1606245 0.1541187 #> [5,] 0.4654485 0.4678403 0.4697874 0.4716071 0.4732987 0.4748713 0.4763227 #>       0.009600  0.008953  0.008349  0.007787  0.007262  0.006772  0.006316 #> [1,] 0.1737529 0.1694131 0.1654163 0.1616866 0.1582222 0.1550038 0.1520134 #> [2,] 0.3414513 0.3429000 0.3442630 0.3455508 0.3467650 0.3479096 0.3489882 #> [3,] 0.2825993 0.2778875 0.2735349 0.2694143 0.2655507 0.2619302 0.2585385 #> [4,] 0.1480914 0.1425139 0.1374375 0.1327272 0.1283855 0.1243830 0.1206917 #> [5,] 0.4776827 0.4789529 0.4801159 0.4812111 0.4822306 0.4831795 0.4840627 #>       0.005890  0.005493  0.005123  0.004778  0.004456  0.004155  0.003875 #> [1,] 0.1492344 0.1466728 0.1444928 0.1424329 0.1405027 0.1386501 0.1369287 #> [2,] 0.3500043 0.3509415 0.3518803 0.3527503 0.3533586 0.3537502 0.3541408 #> [3,] 0.2553621 0.2523529 0.2495729 0.2469461 0.2447719 0.2429714 0.2412345 #> [4,] 0.1172863 0.1141899 0.1115492 0.1090702 0.1068799 0.1049697 0.1031633 #> [5,] 0.4848849 0.4853031 0.4848669 0.4844915 0.4840617 0.4836330 0.4832286 #>       0.003614   0.003371  0.003143   0.002932   0.002734   0.002550   0.002378 #> [1,] 0.1353277 0.13383703 0.1324489 0.13115604 0.12995794 0.12883626 0.12779123 #> [2,] 0.3545055 0.35484554 0.3551627 0.35545839 0.35573412 0.35599163 0.35623163 #> [3,] 0.2396064 0.23808224 0.2366558 0.23532103 0.23408728 0.23291920 0.23182613 #> [4,] 0.1014857 0.09992814 0.0984817 0.09713822 0.09589879 0.09473896 0.09366072 #> [5,] 0.4828457 0.48248384 0.4821423 0.48182006 0.48151451 0.48122843 0.48095934 #>        0.002218   0.002068   0.001929   0.001799   0.001678   0.001565 #> [1,] 0.12681770 0.12591073 0.12507208 0.12428467 0.12355070 0.12286675 #> [2,] 0.35645549 0.35666430 0.35685874 0.35704072 0.35721027 0.35736841 #> [3,] 0.23080405 0.22984851 0.22897016 0.22813544 0.22735452 0.22662478 #> [4,] 0.09265844 0.09172664 0.09086898 0.09006314 0.08931315 0.08861541 #> [5,] 0.48070623 0.48046828 0.48024346 0.48003335 0.47983635 0.47965153 #>        0.001459   0.001361   0.001269  0.001183   0.001104   0.001029 #> [1,] 0.12222938 0.12164196 0.12108838 0.1205721 0.12009081 0.11964223 #> [2,] 0.35751592 0.35765292 0.35778161 0.3579014 0.35801315 0.35811737 #> [3,] 0.22594301 0.22532063 0.22472606 0.2241696 0.22364989 0.22316454 #> [4,] 0.08796622 0.08737086 0.08680872 0.0862849 0.08579724 0.08534322 #> [5,] 0.47947818 0.47931506 0.47916251 0.4790199 0.47888627 0.47876119 #>        0.000960   0.000895   0.000835   0.000779   0.000726   0.000677 #> [1,] 0.11923081 0.11884119 0.11847748 0.11813839 0.11782226 0.11753440 #> [2,] 0.35821374 0.35830477 0.35838947 0.35846842 0.35854206 0.35860968 #> [3,] 0.22272544 0.22230300 0.22190721 0.22153756 0.22119249 0.22088406 #> [4,] 0.08492914 0.08453576 0.08416867 0.08382669 0.08350816 0.08322003 #> [5,] 0.47864413 0.47853423 0.47843169 0.47833582 0.47824616 0.47816299 #>        0.000632   0.000589   0.000549   0.000512   0.000478   0.000446 #> [1,] 0.11725997 0.11700344 0.11676419 0.11654868 0.11634133 0.11614705 #> [2,] 0.35867408 0.35873399 0.35878979 0.35884051 0.35888934 0.35893481 #> [3,] 0.22058456 0.22030331 0.22004056 0.21980961 0.21958260 0.21936851 #> [4,] 0.08294412 0.08268614 0.08244564 0.08223075 0.08202286 0.08182785 #> [5,] 0.47808420 0.47801085 0.47794236 0.47787977 0.47781950 0.47776346 #>        0.000416   0.000388   0.000361   0.000337   0.000314   0.000293 #> [1,] 0.11596575 0.11579667 0.11564698 0.11550103 0.11536370 0.11523538 #> [2,] 0.35897713 0.35901656 0.35905189 0.35908637 0.35911861 0.35914859 #> [3,] 0.21916831 0.21898141 0.21882114 0.21866112 0.21850911 0.21836659 #> [4,] 0.08164589 0.08147627 0.08132755 0.08118166 0.08104407 0.08091546 #> [5,] 0.47771121 0.47766244 0.47761900 0.47757601 0.47753599 0.47749876 #>        0.000273  0.000255   0.000238   0.000222   0.000207   0.000193 #> [1,] 0.11511566 0.1150127 0.11491022 0.11481310 0.11472207 0.11463705 #> [2,] 0.35917651 0.3592009 0.35922520 0.35924809 0.35926939 0.35928921 #> [3,] 0.21823345 0.2181235 0.21801146 0.21790372 0.21780213 0.21770700 #> [4,] 0.08079547 0.0806934 0.08059135 0.08049421 0.08040304 0.08031785 #> [5,] 0.47746405 0.4774345 0.47740398 0.47737534 0.47734874 0.47732398 #>        0.000180   0.000168   0.000156   0.000146   0.000136   0.000127 #> [1,] 0.11456754 0.11448620 0.11442425 0.11436172 0.11430189 0.11427520 #> [2,] 0.35930591 0.35932463 0.35933946 0.35935416 0.35936822 0.35937585 #> [3,] 0.21763314 0.21754030 0.21747314 0.21740465 0.21733813 0.21731280 #> [4,] 0.08024905 0.08016734 0.08010569 0.08004352 0.07998379 0.07995752 #> [5,] 0.47730459 0.47727942 0.47726227 0.47724372 0.47722590 0.47722039 #>       0.000118   0.000110 #> [1,] 0.1141920 0.11416907 #> [2,] 0.3593941 0.35940074 #> [3,] 0.2172166 0.21719396 #> [4,] 0.0798744 0.07985152 #> [5,] 0.4771921 0.47718781 predict(fit, X = X[1:5, ], type = \"response\", lambda = 0.01) #> [1] 0.1764292 0.3405661 0.2854742 0.1515454 0.4768978 predict(fit, X = X[1:5, ], type = \"class\", lambda = 0.01) #> [1] 0 0 0 0 0 predict(fit, X = X[1:5, ], type = \"numnz\", lambda = 0.01) #> 0.0100  #>     18"},{"path":"https://yuyangyy.com/glmtlp/reference/setup_lambda.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate lambda sequence. — setup_lambda","title":"Generate lambda sequence. — setup_lambda","text":"Generate lambda sequence.","code":""},{"path":"https://yuyangyy.com/glmtlp/reference/setup_lambda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate lambda sequence. — setup_lambda","text":"","code":"setup_lambda(X, y, weights, lambda.min.ratio, nlambda)"},{"path":"https://yuyangyy.com/glmtlp/reference/setup_lambda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate lambda sequence. — setup_lambda","text":"X Input matrix, dimension nobs x nvars; row  observation vector. y Response variable, length nobs. family=\"gaussian\", quantitative; family=\"binomial\", either factor two levels binary vector. weights Observation weights. lambda.min.ratio smallest value lambda, fraction lambda.max, smallest value coefficients zero. default depends sample size nobs relative number variables nvars. nlambda number lambda values.","code":""},{"path":"https://yuyangyy.com/glmtlp/news/index.html","id":"glmtlp-202","dir":"Changelog","previous_headings":"","what":"glmtlp 2.0.2","title":"glmtlp 2.0.2","text":"Fix Remapping issue regarding C++. Fix _PACKAGE. Update authors’ information. Fix typos.","code":""},{"path":"https://yuyangyy.com/glmtlp/news/index.html","id":"glmtlp-201","dir":"Changelog","previous_headings":"","what":"glmtlp 2.0.1","title":"glmtlp 2.0.1","text":"CRAN release: 2021-12-17 fix bugs default kappa sequence setting update default hyper-parameter settings update plotting thel0 penalty update R function documentations update core algorithms cv function reduce memory occupation add data simulation functions update data sets add l0 tlp options logistic regression models remove foreach parallel Depends update vignettes README","code":""},{"path":"https://yuyangyy.com/glmtlp/news/index.html","id":"glmtlp-200","dir":"Changelog","previous_headings":"","what":"glmtlp 2.0.0","title":"glmtlp 2.0.0","text":"CRAN release: 2021-10-15 new version glmtlp. - compatible previous version 1.1. - uses C++ core.","code":""}]
